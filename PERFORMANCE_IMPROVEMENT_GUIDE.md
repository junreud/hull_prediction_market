# ğŸš€ Performance Improvement Guide

## í˜„ì¬ ìƒí™© ìš”ì•½

### ì™„ë£Œëœ ìµœì í™”
- âœ… **LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„°**: Phase 3ì—ì„œ Optunaë¡œ ìµœì í™” (ê³ ì •ê°’ìœ¼ë¡œ ê°„ì£¼)
- âœ… **Position ì „ëµ íŒŒë¼ë¯¸í„°**: Phase 4ì—ì„œ ìµœì í™” (Quantile 7-bin allocations)
- âœ… **Ensemble ê°€ì¤‘ì¹˜**: Phase 6ì—ì„œ ìµœì í™” ([77.9%, 14.8%, 7.2%])
- âœ… **í˜„ì¬ ì„±ëŠ¥**: Ensemble RMSE 0.010540, Correlation 0.0229 (stacking)

### íŒŒë¼ë¯¸í„° ì¬íŠœë‹ì˜ í•œê³„
ìœ„ 3ê°€ì§€ë¥¼ **ì¬íŠœë‹í•´ë„ ì„±ëŠ¥ ê°œì„ ì€ ë§¤ìš° ì œí•œì **ì…ë‹ˆë‹¤ (ì´ë¯¸ ìµœì í™”ë¨).
ì‹¤ì§ˆì  ê°œì„ ì€ **Feature Engineering, Model Diversity, Risk Modeling**ì—ì„œ ë‚˜ì˜µë‹ˆë‹¤.

---

## ğŸ¯ ì„±ëŠ¥ ê°œì„  ìš°ì„ ìˆœìœ„

### 1ï¸âƒ£ Feature Engineering (ê°€ì¥ ì¤‘ìš”! ì„±ëŠ¥ì˜ 80% ê²°ì •)

#### í˜„ì¬ ìƒíƒœ
```python
# í˜„ì¬ feature í™•ì¸
results/feature_selection/selected_features_optimized.csv      # ì„ íƒëœ feature
artifacts/lightgbm_feature_importance.csv                      # ì¤‘ìš”ë„
```

#### ê°œì„  ë°©í–¥

**A. ìƒí˜¸ì‘ìš© Feature**
```python
# features.pyì— ì¶”ê°€
def create_interaction_features(df):
    """ì‹œì¥ì§€í‘œ Ã— ë³€ë™ì„± ìƒí˜¸ì‘ìš©"""
    # ì‹œì¥ momentum Ã— ë³€ë™ì„± â†’ ì¶”ì„¸ ê°•ë„
    df['M_V_interaction'] = df['M_momentum'] * df['V_realized_vol']
    
    # ê¸ˆë¦¬ Ã— ê°€ê²© â†’ ë°¸ë¥˜ì—ì´ì…˜ ì¡°ì •
    df['I_P_interaction'] = df['I_fed_rate'] * df['P_pe_ratio']
    
    # ê±°ì‹œê²½ì œ Ã— ì‹œì¥ â†’ ê²½ê¸° ì‚¬ì´í´
    df['E_M_interaction'] = df['E_gdp_growth'] * df['M_market_return']
    
    return df
```

**B. ë ˆì§ Feature**
```python
def create_regime_features(df):
    """ì‹œì¥ ë ˆì§ ë¶„ë¥˜ feature"""
    # ë³€ë™ì„± ë ˆì§ (ê³ /ì¤‘/ì €)
    df['regime_vol'] = pd.qcut(df['V_realized_vol'], q=3, labels=[0, 1, 2])
    
    # ì¶”ì„¸ ë ˆì§ (ìƒìŠ¹/íš¡ë³´/í•˜ë½)
    rolling_return = df['M_market_return'].rolling(20).mean()
    df['regime_trend'] = pd.cut(rolling_return, bins=[-np.inf, -0.01, 0.01, np.inf], 
                                  labels=[0, 1, 2])
    
    # ê³µí¬ ë ˆì§ (VIX ëŒ€ìš©)
    df['regime_fear'] = (df['V_vix'] > df['V_vix'].rolling(60).quantile(0.75)).astype(int)
    
    return df
```

**C. ë„ë©”ì¸ ì§€ì‹ Feature**
```python
def create_technical_features(df):
    """ê¸ˆìœµ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ feature"""
    # RSI (Relative Strength Index)
    delta = df['M_market_return'].diff()
    gain = delta.where(delta > 0, 0).rolling(14).mean()
    loss = -delta.where(delta < 0, 0).rolling(14).mean()
    df['rsi'] = 100 - (100 / (1 + gain / (loss + 1e-10)))
    
    # Bollinger Bands
    rolling_mean = df['M_market_return'].rolling(20).mean()
    rolling_std = df['M_market_return'].rolling(20).std()
    df['bollinger_upper'] = rolling_mean + 2 * rolling_std
    df['bollinger_lower'] = rolling_mean - 2 * rolling_std
    df['bollinger_position'] = (df['M_market_return'] - rolling_mean) / (rolling_std + 1e-10)
    
    # MACD (Moving Average Convergence Divergence)
    ema_12 = df['M_market_return'].ewm(span=12).mean()
    ema_26 = df['M_market_return'].ewm(span=26).mean()
    df['macd'] = ema_12 - ema_26
    df['macd_signal'] = df['macd'].ewm(span=9).mean()
    
    return df
```

**D. ì‹œê³„ì—´ Feature ë‹¤ì–‘í™”**
```python
def create_temporal_features(df):
    """ë‹¤ì–‘í•œ ì‹œê°„ ìœˆë„ìš° feature"""
    windows = [5, 10, 20, 40, 60, 120]
    
    for window in windows:
        # ë¡¤ë§ í‰ê· 
        df[f'M_return_ma_{window}'] = df['M_market_return'].rolling(window).mean()
        
        # ë¡¤ë§ í‘œì¤€í¸ì°¨
        df[f'M_return_std_{window}'] = df['M_market_return'].rolling(window).std()
        
        # ë¡¤ë§ ìµœëŒ€/ìµœì†Œ
        df[f'M_return_max_{window}'] = df['M_market_return'].rolling(window).max()
        df[f'M_return_min_{window}'] = df['M_market_return'].rolling(window).min()
        
        # í˜„ì¬ê°’ vs ë¡¤ë§ í‰ê·  ë¹„ìœ¨
        df[f'M_return_ratio_{window}'] = df['M_market_return'] / (df[f'M_return_ma_{window}'] + 1e-10)
    
    return df
```

#### ì‹¤í—˜ í”„ë¡œì„¸ìŠ¤
```bash
# 1. features.py ìˆ˜ì • (ìœ„ í•¨ìˆ˜ ì¶”ê°€)
vim src/features.py

# 2. Feature selection ì¬ì‹¤í–‰
python scripts/test_features.py

# 3. ëª¨ë¸ ì¬í•™ìŠµ
python scripts/test_model_training.py

# 4. ì„±ëŠ¥ ë¹„êµ
# Before: RMSE 0.010540
# After:  RMSE 0.010xxx (ê°œì„  í™•ì¸)

# 5. ê°œì„ ë˜ë©´ keep, ì•„ë‹ˆë©´ drop
```

---

### 2ï¸âƒ£ Model Diversity (Ensemble ë‹¤ì–‘ì„± í™•ë³´)

#### í˜„ì¬ ìƒíƒœ
```python
# í˜„ì¬: 3ê°œ LightGBM ëª¨ë¸ë§Œ ì•™ìƒë¸”
# - standard (optimized params)
# - complex (deeper trees)
# - regularized (L1/L2)
```

#### ê°œì„  ë°©í–¥

**A. ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ ì¶”ê°€**
```python
# scripts/optimize_ensemble.py ìˆ˜ì •

from catboost import CatBoostRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import Ridge

def train_diverse_models_v2(df, target_col, date_col, config_path):
    """ë” ë‹¤ì–‘í•œ ëª¨ë¸ í•™ìŠµ"""
    
    # 1. LightGBM (ê¸°ì¡´)
    lgbm_predictor = ReturnPredictor(config_path=config_path)
    lgbm_result = lgbm_predictor.train_cv(df, target_col, date_col)
    
    # 2. CatBoost (NEW)
    catboost_params = {
        'iterations': 200,
        'depth': 6,
        'learning_rate': 0.05,
        'l2_leaf_reg': 3.0,
        'random_seed': 42,
        'verbose': False
    }
    catboost_predictor = CatBoostPredictor(params=catboost_params)
    catboost_result = catboost_predictor.train_cv(df, target_col, date_col)
    
    # 3. XGBoost (NEW)
    xgb_params = {
        'n_estimators': 200,
        'max_depth': 5,
        'learning_rate': 0.05,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'random_state': 42
    }
    xgb_predictor = XGBPredictor(params=xgb_params)
    xgb_result = xgb_predictor.train_cv(df, target_col, date_col)
    
    # 4. Ridge (Linear for diversity)
    ridge_predictor = RidgePredictor(alpha=1.0)
    ridge_result = ridge_predictor.train_cv(df, target_col, date_col)
    
    return {
        'lgbm': lgbm_result,
        'catboost': catboost_result,
        'xgb': xgb_result,
        'ridge': ridge_result
    }
```

**B. ë‹¤ë¥¸ Feature Setìœ¼ë¡œ í•™ìŠµ**
```python
def train_feature_subset_models(df, target_col, date_col):
    """ì„œë¡œ ë‹¤ë¥¸ feature ì¡°í•©ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµ"""
    
    # Feature ê·¸ë£¹ ë¶„ë¦¬
    M_features = [col for col in df.columns if col.startswith('M_')]
    V_features = [col for col in df.columns if col.startswith('V_')]
    E_features = [col for col in df.columns if col.startswith('E_')]
    I_features = [col for col in df.columns if col.startswith('I_')]
    P_features = [col for col in df.columns if col.startswith('P_')]
    
    # Model A: ì‹œì¥/ë³€ë™ì„± ìœ„ì£¼
    df_market = df[M_features + V_features + [target_col, date_col]]
    model_A = train_model(df_market, target_col, date_col)
    
    # Model B: ê±°ì‹œê²½ì œ/ê¸ˆë¦¬/ê°€ê²© ìœ„ì£¼
    df_macro = df[E_features + I_features + P_features + [target_col, date_col]]
    model_B = train_model(df_macro, target_col, date_col)
    
    # Model C: ì „ì²´ feature (ê¸°ì¡´)
    model_C = train_model(df, target_col, date_col)
    
    return {'market_vol': model_A, 'macro': model_B, 'all': model_C}
```

**C. ë‹¤ë¥¸ CV ì „ëµ**
```python
def train_different_cv_models(df, target_col, date_col):
    """ë‹¤ë¥¸ CV ì „ëµìœ¼ë¡œ ëª¨ë¸ í•™ìŠµ"""
    
    # Model A: 4-fold CV (í˜„ì¬)
    model_A = train_cv(df, target_col, date_col, n_splits=4)
    
    # Model B: 5-fold CV
    model_B = train_cv(df, target_col, date_col, n_splits=5)
    
    # Model C: Time-series split (expanding window)
    model_C = train_expanding_cv(df, target_col, date_col)
    
    return {'cv4': model_A, 'cv5': model_B, 'expanding': model_C}
```

---

### 3ï¸âƒ£ Risk Model ê°œì„ 

#### í˜„ì¬ ìƒíƒœ
```python
# í˜„ì¬: LightGBM regression for rolling_std(forward_returns, 20)
# artifacts/oof_sigma_hat.csv
```

#### ê°œì„  ë°©í–¥

**A. GARCH ëª¨ë¸ ì¶”ê°€**
```python
# src/risk.pyì— ì¶”ê°€

from arch import arch_model

class GARCHRiskModel:
    """GARCH(1,1) ë³€ë™ì„± ì˜ˆì¸¡"""
    
    def predict_volatility(self, returns, horizon=1):
        """
        GARCH(1,1) ëª¨ë¸ë¡œ ë³€ë™ì„± ì˜ˆì¸¡
        
        ÏƒÂ²_t = Ï‰ + Î±Â·ÎµÂ²_{t-1} + Î²Â·ÏƒÂ²_{t-1}
        """
        model = arch_model(returns, vol='GARCH', p=1, q=1)
        fitted = model.fit(disp='off')
        forecast = fitted.forecast(horizon=horizon)
        
        # Annualized volatility -> Daily volatility
        sigma_hat = np.sqrt(forecast.variance.values[-1, 0])
        return sigma_hat
```

**B. EWMA (Exponentially Weighted Moving Average)**
```python
class EWMARiskModel:
    """ì§€ìˆ˜ê°€ì¤‘ ì´ë™í‰ê·  ë³€ë™ì„±"""
    
    def __init__(self, span=20):
        self.span = span
    
    def predict_volatility(self, returns):
        """EWMA volatility"""
        return returns.ewm(span=self.span).std()
```

**C. Risk Ensemble (Conservative)**
```python
def ensemble_risk_predictions(lgbm_risk, garch_risk, ewma_risk):
    """ë³´ìˆ˜ì  ë¦¬ìŠ¤í¬ ì•™ìƒë¸” (ê³¼ì†Œì¶”ì • ë°©ì§€)"""
    
    # ì „ëµ 1: Max (ê°€ì¥ ë³´ìˆ˜ì )
    risk_max = np.maximum.reduce([lgbm_risk, garch_risk, ewma_risk])
    
    # ì „ëµ 2: Weighted average (í¸í–¥ ë³´ì •)
    risk_weighted = 0.5 * lgbm_risk + 0.3 * garch_risk + 0.2 * ewma_risk
    
    # ì „ëµ 3: 75th percentile (ê·¹ë‹¨ê°’ ë°°ì œ)
    risk_75th = np.percentile([lgbm_risk, garch_risk, ewma_risk], 75, axis=0)
    
    # ìµœì¢…: Maxì™€ Weightedì˜ í‰ê·  (ì•ˆì „ì„± + ì •í™•ì„±)
    final_risk = 0.6 * risk_max + 0.4 * risk_weighted
    
    return final_risk
```

---

### 4ï¸âƒ£ Position Strategy ë‹¤ì–‘í™” (ì„ íƒ)

#### í˜„ì¬ ìƒíƒœ
```python
# í˜„ì¬: Quantile Binningë§Œ ì‚¬ìš©
# allocations: [0.00023, 0.0039, 0.087, 0.74, 0.90, 1.08, 1.85]
```

#### ê°œì„  ë°©í–¥

**A. Kelly Criterion ì¶”ê°€**
```python
class KellyCriterionMapper(BasePositionMapper):
    """ì¼ˆë¦¬ ê¸°ì¤€ í¬ì§€ì…˜ ë§¤í•‘"""
    
    def map_positions(self, r_hat, sigma_hat):
        """
        Kelly Fraction: f* = Î¼ / ÏƒÂ²
        
        Full KellyëŠ” ë³€ë™ì„±ì´ ë„ˆë¬´ í¬ë¯€ë¡œ Half Kelly ì‚¬ìš©
        """
        kelly_fraction = r_hat / (sigma_hat ** 2 + 1e-10)
        
        # Half Kelly (ì•ˆì „)
        half_kelly = 0.5 * kelly_fraction
        
        # Clip to [0, 2]
        positions = np.clip(1 + half_kelly, 0, 2)
        
        return positions
```

**B. ì „ëµ ì•™ìƒë¸”**
```python
def ensemble_positions(r_hat, sigma_hat):
    """ì—¬ëŸ¬ ì „ëµ ì•™ìƒë¸”"""
    
    # Strategy 1: Quantile Binning (í˜„ì¬)
    quantile_mapper = QuantileBinningMapper(config_path="conf/params.yaml")
    pos_quantile = quantile_mapper.map_positions(r_hat, sigma_hat)
    
    # Strategy 2: Sharpe Scaling
    sharpe_mapper = SharpeScalingMapper(config_path="conf/params.yaml")
    pos_sharpe = sharpe_mapper.map_positions(r_hat, sigma_hat)
    
    # Strategy 3: Kelly Criterion
    kelly_mapper = KellyCriterionMapper()
    pos_kelly = kelly_mapper.map_positions(r_hat, sigma_hat)
    
    # ê°€ì¤‘ í‰ê·  (Optunaë¡œ ìµœì í™” ê°€ëŠ¥)
    final_position = 0.5 * pos_quantile + 0.3 * pos_sharpe + 0.2 * pos_kelly
    
    return np.clip(final_position, 0, 2)
```

---

### 5ï¸âƒ£ ë°ì´í„° í’ˆì§ˆ ê°œì„ 

#### í˜„ì¬ ìƒíƒœ
```python
# ê²°ì¸¡ì¹˜: forward-fill + median
# ì´ìƒì¹˜: MAD ê¸°ì¤€ ìœˆì €ë¼ì´ì œì´ì…˜
# Scaling: RobustScaler
```

#### ê°œì„  ë°©í–¥

**A. ê³ ê¸‰ ê²°ì¸¡ì¹˜ ì²˜ë¦¬**
```python
from sklearn.impute import KNNImputer

def advanced_imputation(df):
    """ê·¸ë£¹ë³„ KNN imputation"""
    
    # Economic features: KNN imputation
    E_features = [col for col in df.columns if col.startswith('E_')]
    imputer = KNNImputer(n_neighbors=5)
    df[E_features] = imputer.fit_transform(df[E_features])
    
    # Market features: Linear interpolation
    M_features = [col for col in df.columns if col.startswith('M_')]
    df[M_features] = df[M_features].interpolate(method='linear', limit_direction='both')
    
    return df
```

**B. ë ˆì§ë³„ ìŠ¤ì¼€ì¼ë§**
```python
def regime_based_scaling(df):
    """ë³€ë™ì„± ë ˆì§ë³„ë¡œ ë‹¤ë¥¸ scaling"""
    
    # ë³€ë™ì„± ë ˆì§ ë¶„ë¥˜
    vol = df['V_realized_vol'].rolling(20).mean()
    df['regime'] = pd.qcut(vol, q=3, labels=['low', 'mid', 'high'])
    
    # ë ˆì§ë³„ scaling
    scaler_low = RobustScaler()
    scaler_mid = RobustScaler()
    scaler_high = RobustScaler()
    
    df.loc[df['regime'] == 'low', features] = scaler_low.fit_transform(
        df.loc[df['regime'] == 'low', features]
    )
    # mid, highë„ ë™ì¼í•˜ê²Œ...
    
    return df
```

---

## ğŸ“‹ Phase 6-7 ì¶”ê°€ ì‘ì—…

### Phase 6 ì™„ë£Œ ë¬¸ì„œ
```bash
# PHASE6_COMPLETE.md ì‘ì„±
- Ensemble ì „ëµ 4ê°œ ë¹„êµ ê²°ê³¼
- ìµœì  ê°€ì¤‘ì¹˜: [77.9%, 14.8%, 7.2%]
- ì„±ëŠ¥: Single model 0.011168 â†’ Ensemble 0.010540 (5.6% ê°œì„ )
- Correlation: 0.0145 â†’ 0.0229 (57% ê°œì„ )
```

### Phase 7 ë°±í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸
```python
# scripts/backtest.py ìƒì„±
def backtest_strategy(allocations, forward_returns):
    """
    ì „ëµ ì‹œë®¬ë ˆì´ì…˜ ë° ë¦¬í¬íŠ¸
    
    Returns
    -------
    report : dict
        - sharpe_ratio
        - annual_return
        - annual_volatility
        - max_drawdown
        - turnover
        - constraint_violation_rate (Ïƒ_strategy/Ïƒ_market > 1.2 ë¹„ìœ¨)
        - leverage_usage (2ë°° ë ˆë²„ë¦¬ì§€ ì‚¬ìš© ë¹„ìœ¨)
    """
    pass
```

### Kaggle ì œì¶œ ë…¸íŠ¸ë¶
```python
# notebooks/30_final_train_submit.ipynb
# 
# 1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
# 2. Feature engineering
# 3. Return model í•™ìŠµ ë° ì˜ˆì¸¡
# 4. Risk model í•™ìŠµ ë° ì˜ˆì¸¡
# 5. Ensemble ì˜ˆì¸¡
# 6. Position mapping
# 7. submission.csv ìƒì„±
```

### ëŸ°íƒ€ì„ ìµœì í™”
```bash
# ëª©í‘œ: 8ì‹œê°„ ì´ë‚´ ì™„ë£Œ
# 
# 1. ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
python -m memory_profiler scripts/optimize_ensemble.py

# 2. ë¶ˆí•„ìš”í•œ feature ì œê±°
# feature importance < 0.001 ì œê±°

# 3. ëª¨ë¸ ìˆ˜ ì¡°ì •
# í˜„ì¬: 3ê°œ ëª¨ë¸ ì•™ìƒë¸”
# ìµœì : 2ê°œë¡œ ì¶•ì†Œ ê°€ëŠ¥ (standard + complex)

# 4. CV fold ìˆ˜ ì¡°ì •
# í˜„ì¬: 4-fold
# ê³ ë ¤: 3-fold (20% ì‹œê°„ ë‹¨ì¶•)
```

### ì œì•½ ì¡°ê±´ ìµœì¢… ê²€ì¦
```python
# ë°˜ë“œì‹œ í™•ì¸
constraints = {
    'vol_ratio_violation': (Ïƒ_strategy / Ïƒ_market > 1.2).mean() <= 0.02,  # â‰¤2%
    'leverage_usage': (allocations >= 1.9).mean() <= 0.10,                # â‰¤10%
    'underperformance': mean(strategy_returns) >= mean(market_returns),   # No penalty
}
```

---

## ğŸ” ì‹¤í—˜ ì¶”ì 

### MLflow ì„¤ì •
```python
import mlflow

# ì‹¤í—˜ ì¶”ì 
with mlflow.start_run():
    mlflow.log_params({
        'n_models': 3,
        'ensemble_strategy': 'stacking',
        'position_strategy': 'quantile_binning'
    })
    
    mlflow.log_metrics({
        'rmse': 0.010540,
        'correlation': 0.0229,
        'sharpe': 0.0308
    })
    
    mlflow.log_artifact('artifacts/ensemble_config.json')
```

---

## âš¡ Quick Wins (ë¹ ë¥¸ ì„±ëŠ¥ ê°œì„ )

ìš°ì„ ìˆœìœ„ ë†’ì€ ê²ƒë¶€í„°:

1. **ìƒí˜¸ì‘ìš© feature ì¶”ê°€** (30ë¶„) â†’ ì˜ˆìƒ ê°œì„ : RMSE 2-3%
2. **CatBoost ëª¨ë¸ ì¶”ê°€** (1ì‹œê°„) â†’ ì˜ˆìƒ ê°œì„ : RMSE 1-2%
3. **GARCH risk model ì¶”ê°€** (1ì‹œê°„) â†’ ì˜ˆìƒ ê°œì„ : Sharpe 5-10%
4. **ë ˆì§ feature ì¶”ê°€** (1ì‹œê°„) â†’ ì˜ˆìƒ ê°œì„ : RMSE 1-2%
5. **Position strategy ì•™ìƒë¸”** (30ë¶„) â†’ ì˜ˆìƒ ê°œì„ : Sharpe 3-5%

---

## ğŸ“š ì°¸ê³  ìë£Œ

- Feature Engineering: `/notebooks/01_feature_group_detailed_analysis.ipynb`
- Current Features: `/results/feature_selection/selected_features_optimized.csv`
- Ensemble Results: `/artifacts/ensemble_comparison.csv`
- Position Strategy: `/src/position.py`
- Risk Model: `/src/risk.py`
