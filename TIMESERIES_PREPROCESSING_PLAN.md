# ğŸ“‹ Time-Series Preprocessing Implementation Plan

**ì‘ì„±ì¼**: 2025-11-12  
**ëª©í‘œ**: ê¸ˆìœµ ì‹œê³„ì—´ ë°ì´í„°ì— íŠ¹í™”ëœ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

---

## ğŸ¯ í•µì‹¬ ì›ì¹™

1. **ë¯¸ë˜ì •ë³´ ê¸ˆì§€**: ëª¨ë“  ì—°ì‚°ì€ `t-1` ì‹œì ê¹Œì§€ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©
2. **ë°œí‘œì£¼ê¸° ì¡´ì¤‘**: ì›”/ë¶„ê¸° ì§€í‘œëŠ” ë°œí‘œì¼ ê¸°ì¤€ forward-fillë§Œ
3. **ë³€ë™ì„± ì ì‘**: ë ˆì§ë³„ ì°¨ë“± ì²˜ë¦¬ (ê³ ë³€ë™ê¸° â‰  ì €ë³€ë™ê¸°)
4. **ì •ë³´ ì••ì¶•**: ì¤‘ë³µ í”¼ì²˜ëŠ” PCA/í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ì¶•ì†Œ

---

## ğŸ“Š í˜„ì¬ ìƒíƒœ ë¶„ì„

### âœ… ì´ë¯¸ êµ¬í˜„ëœ ê¸°ëŠ¥

| ê¸°ëŠ¥ | íŒŒì¼ | ìƒíƒœ |
|------|------|------|
| ì‹œê³„ì—´ ì¸ì§€ ê²°ì¸¡ì¹˜ ë³´ê°„ | `data.py::handle_missing_values` | âœ… interpolate/EWMA |
| ë¡¤ë§ ìœˆë„ìš° ì´ìƒì¹˜ íƒì§€ | `data.py::detect_outliers` | âœ… rolling_mad/rolling_iqr/ewma |
| ë¡¤ë§ winsorization | `data.py::winsorize_outliers` | âœ… rolling method |
| ë ˆì§ ê°ì§€ | `data.py::detect_regime_changes` | âœ… ë³€ë™ì„± ê¸°ë°˜ ë¶„ë¥˜ |
| í†µí•© íŒŒì´í”„ë¼ì¸ | `data.py::preprocess_timeseries` | âœ… ì „ì²´ íë¦„ |

### âŒ ëˆ„ë½ëœ ê¸°ëŠ¥ (Critical)

| ê¸°ëŠ¥ | ì¤‘ìš”ë„ | í˜„ì¬ ë¬¸ì œ |
|------|--------|-----------|
| ë°œí‘œì¼ ì •ë ¬ | ğŸ”´ Critical | E ê·¸ë£¹ ì„ í˜•ë³´ê°„ ì‹œ ë¯¸ë˜ì •ë³´ ìœ ì¶œ ê°€ëŠ¥ |
| ê²°ì¸¡ ë§ˆìŠ¤í¬ í”¼ì²˜ | ğŸ”´ Critical | ê²°ì¸¡ ìì²´ê°€ ì‹ í˜¸ì¸ë° ì •ë³´ ì†ì‹¤ |
| ê·¸ë£¹ë³„ ìµœëŒ€ í—ˆìš© ê³µë°± | ğŸŸ¡ Important | ì¥ê¸° ê²°ì¸¡ ì‹œ ë¬´ë¦¬í•œ ë³´ê°„ |
| ê³µì„ ì„± ì²˜ë¦¬ (PCA) | ğŸŸ¡ Important | V ê·¸ë£¹ 13ê°œê°€ ìœ ì‚¬í•˜ë©´ ê³¼ì í•© |
| ë ˆì§ ê¸°ë°˜ ê°€ì¤‘ì¹˜ | ğŸŸ¢ Nice-to-have | ê³ ë³€ë™ê¸° ë…¸ì´ì¦ˆ ì™„í™” |
| ì´ë²¤íŠ¸ ë”ë¯¸ ë³€ìˆ˜ | ğŸŸ¢ Nice-to-have | ë°œí‘œì¼ ê¸‰ë“±ë½ ë³´ì¡´ |

---

## ğŸ› ï¸ êµ¬í˜„ ê³„íš

### Phase 1: Critical ê¸°ëŠ¥ (ì¦‰ì‹œ êµ¬í˜„)

#### 1.1 ë°œí‘œì¼ ì •ë ¬ ê¸°ëŠ¥ ì¶”ê°€

**ëª©í‘œ**: ê±°ì‹œì§€í‘œ(E ê·¸ë£¹)ì˜ ë°œí‘œ ì‹œì°¨ë¥¼ ë°˜ì˜

```python
def align_announcement_dates(
    self,
    df: pd.DataFrame,
    announcement_calendar: Optional[pd.DataFrame] = None
) -> pd.DataFrame:
    """
    Align economic indicators with their announcement dates.
    
    Args:
        df: DataFrame with date_id
        announcement_calendar: DataFrame with columns:
            - feature: Feature name (e.g., 'E1')
            - announcement_date: Actual announcement date_id
            - reference_period: Period the value refers to
    
    Returns:
        DataFrame with properly aligned values
    """
    pass
```

**êµ¬í˜„ ìœ„ì¹˜**: `DataLoader` í´ë˜ìŠ¤ ë‚´  
**ì ìš© ê·¸ë£¹**: E (Economic), ì¼ë¶€ P (Valuation)

#### 1.2 ê²°ì¸¡ ë§ˆìŠ¤í¬ í”¼ì²˜ ìƒì„±

**ëª©í‘œ**: ê²°ì¸¡ ì—¬ë¶€ì™€ ê²°ì¸¡ ì§€ì† ê¸°ê°„ì„ í”¼ì²˜ë¡œ ì¶”ê°€

```python
def create_missing_masks(
    self,
    df: pd.DataFrame,
    suffix: str = '_is_missing'
) -> pd.DataFrame:
    """
    Create binary missing indicators and gap duration features.
    
    For each feature with missing values, creates:
    1. {feature}_is_missing: Binary indicator
    2. {feature}_missing_days: Days since last valid observation
    
    Args:
        df: DataFrame to process
        suffix: Suffix for mask columns
    
    Returns:
        DataFrame with additional mask features
    """
    df = df.copy()
    
    for group, features in self.feature_groups.items():
        if group == 'D':  # Skip dummy variables
            continue
            
        for col in features:
            if col not in df.columns:
                continue
                
            # Binary missing indicator
            mask_col = f"{col}{suffix}"
            df[mask_col] = df[col].isna().astype(int)
            
            # Missing duration (days since last valid value)
            duration_col = f"{col}_missing_days"
            is_missing = df[col].isna()
            
            # Calculate cumulative days missing
            missing_counter = 0
            duration_values = []
            
            for missing in is_missing:
                if missing:
                    missing_counter += 1
                else:
                    missing_counter = 0
                duration_values.append(missing_counter)
            
            df[duration_col] = duration_values
    
    return df
```

#### 1.3 ê·¸ë£¹ë³„ ë³´ê°„ ì „ëµ ê°•í™”

**í˜„ì¬ ë¬¸ì œ**: E/I/P ê·¸ë£¹ì— `interpolate` ì‚¬ìš© ì¤‘ â†’ ë¯¸ë˜ì •ë³´ ìœ„í—˜

**ìˆ˜ì •ì•ˆ**:
```python
# data.py::handle_missing_values ìˆ˜ì •
default_strategy = {
    'E': 'locf',        # âŒ interpolate â†’ âœ… LOCF only
    'I': 'locf_median', # Interest rates: LOCF + fallback median
    'P': 'locf',        # Price/Valuation: LOCF only
    'M': 'ewma',        # Market: EWMA (í˜„ì¬ì™€ ë™ì¼)
    'V': 'ewma',        # Volatility: EWMA
    'S': 'ewma',        # Sentiment: EWMA
    'D': 'zero',
}

# max_gap íŒŒë¼ë¯¸í„° ì¶”ê°€
if group_strategy == 'locf':
    for col in group_features:
        # LOCF with max gap limit
        df[col] = df[col].fillna(method='ffill', limit=max_gap)
        
        # Fallback: training median (ë¯¸ë˜ì •ë³´ ì—†ìŒ)
        if train_df is not None:
            fallback = train_df[col].median()
        else:
            fallback = df[col].median()
        df[col] = df[col].fillna(fallback)
```

---

### Phase 2: Important ê¸°ëŠ¥ (1ì£¼ì¼ ë‚´)

#### 2.1 ê³µì„ ì„± ì²˜ë¦¬ - ìƒê´€ í´ëŸ¬ìŠ¤í„°ë§

**ëª©í‘œ**: ìœ ì‚¬í•œ í”¼ì²˜ë¥¼ í´ëŸ¬ìŠ¤í„°ë§í•˜ê³  ëŒ€í‘œ í”¼ì²˜ë§Œ ì„ íƒ

```python
def detect_feature_clusters(
    self,
    df: pd.DataFrame,
    method: str = 'correlation',
    threshold: float = 0.85,
    by_group: bool = True
) -> Dict[str, List[List[str]]]:
    """
    Detect highly correlated feature clusters.
    
    Args:
        df: DataFrame to analyze
        method: 'correlation' or 'distance'
        threshold: Correlation threshold for clustering
        by_group: Whether to cluster within feature groups
    
    Returns:
        Dictionary mapping group -> list of clusters
    """
    pass

def reduce_feature_redundancy(
    self,
    df: pd.DataFrame,
    clusters: Dict[str, List[List[str]]],
    method: str = 'representative'
) -> pd.DataFrame:
    """
    Reduce feature redundancy using clustering results.
    
    Methods:
    - 'representative': Keep only the feature with highest variance
    - 'mean': Replace cluster with mean of z-scores
    - 'pca': Replace cluster with first principal component
    
    Args:
        df: DataFrame to process
        clusters: Output from detect_feature_clusters
        method: Reduction method
    
    Returns:
        DataFrame with reduced features
    """
    pass
```

#### 2.2 ê·¸ë£¹ë³„ PCA

**ëª©í‘œ**: ê° í”¼ì²˜ ê·¸ë£¹ ë‚´ì—ì„œ ì°¨ì› ì¶•ì†Œ

```python
def apply_group_pca(
    self,
    df: pd.DataFrame,
    train_df: Optional[pd.DataFrame] = None,
    n_components: Dict[str, int] = None,
    variance_threshold: float = 0.95
) -> Tuple[pd.DataFrame, Dict]:
    """
    Apply PCA within each feature group.
    
    Args:
        df: DataFrame to transform
        train_df: Training data for fitting PCA
        n_components: Number of components per group
        variance_threshold: Cumulative variance to retain
    
    Returns:
        Tuple of (transformed DataFrame, PCA models dict)
    
    Example:
        >>> n_components = {
        >>>     'M': 3,  # 18 features -> 3 components
        >>>     'V': 2,  # 13 features -> 2 components
        >>> }
    """
    from sklearn.decomposition import PCA
    
    df = df.copy()
    pca_models = {}
    
    for group, features in self.feature_groups.items():
        if group == 'D' or not features:
            continue
            
        # Determine n_components
        if n_components and group in n_components:
            n = n_components[group]
        else:
            # Auto-determine based on variance threshold
            n = 'auto'
        
        # Fit PCA on training data
        fit_df = train_df if train_df is not None else df
        # ... implementation ...
    
    return df, pca_models
```

#### 2.3 ë ˆì§ ê¸°ë°˜ ìƒ˜í”Œ ê°€ì¤‘ì¹˜

**ëª©í‘œ**: ê³ ë³€ë™ì„± êµ¬ê°„ì˜ ìƒ˜í”Œì— ë‚®ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬

```python
def calculate_regime_weights(
    self,
    df: pd.DataFrame,
    regime_col: str = 'regime',
    weight_map: Optional[Dict[str, float]] = None
) -> pd.Series:
    """
    Calculate sample weights based on volatility regime.
    
    Default weights:
    - Low Vol: 1.0 (standard weight)
    - Normal: 1.0
    - High Vol: 0.5 (reduced weight due to noise)
    
    Args:
        df: DataFrame with regime column
        regime_col: Name of regime column
        weight_map: Custom weight mapping
    
    Returns:
        Series of sample weights
    """
    if weight_map is None:
        weight_map = {
            'low_vol': 1.0,
            'normal': 1.0,
            'high_vol': 0.5
        }
    
    weights = df[regime_col].map(weight_map)
    weights = weights.fillna(1.0)  # Unknown regime -> normal weight
    
    return weights
```

---

### Phase 3: Nice-to-have ê¸°ëŠ¥ (2ì£¼ì¼ ë‚´)

#### 3.1 ì´ë²¤íŠ¸ ë”ë¯¸ ë³€ìˆ˜

**ëª©í‘œ**: FOMC, ì‹¤ì ë°œí‘œ ë“± ì´ë²¤íŠ¸ êµ¬ê°„ í‘œì‹œ

```python
def add_event_dummies(
    self,
    df: pd.DataFrame,
    event_calendar: pd.DataFrame
) -> pd.DataFrame:
    """
    Add binary event indicators.
    
    Args:
        df: DataFrame with date_id
        event_calendar: DataFrame with columns:
            - event_date: date_id of event
            - event_type: 'FOMC', 'earnings', 'CPI', etc.
            - window_before: Days before event
            - window_after: Days after event
    
    Returns:
        DataFrame with event dummy columns
    """
    pass
```

#### 3.2 Granger Causality ë¶„ì„

**ëª©í‘œ**: í”¼ì²˜ ê°„ ì‹œì°¨ ì¸ê³¼ê´€ê³„ íƒìƒ‰

```python
def analyze_granger_causality(
    self,
    df: pd.DataFrame,
    target: str = 'forward_returns',
    max_lag: int = 5,
    significance: float = 0.05
) -> pd.DataFrame:
    """
    Test Granger causality between features and target.
    
    Args:
        df: DataFrame to analyze
        target: Target variable
        max_lag: Maximum lag to test
        significance: P-value threshold
    
    Returns:
        DataFrame with causality test results
    """
    from statsmodels.tsa.stattools import grangercausalitytests
    # ... implementation ...
```

---

## ğŸ“ íŒŒì¼ êµ¬ì¡° ë³€ê²½

### ê¸°ì¡´ êµ¬ì¡°
```
src/
  â”œâ”€â”€ data.py              # ì „ì²˜ë¦¬ ì „ì²´
  â”œâ”€â”€ cv.py                # Cross-validation
  â””â”€â”€ utils.py             # ìœ í‹¸ë¦¬í‹°
```

### ì œì•ˆ êµ¬ì¡°
```
src/
  â”œâ”€â”€ data.py              # ê¸°ë³¸ ë°ì´í„° ë¡œë”©
  â”œâ”€â”€ preprocessing/
  â”‚   â”œâ”€â”€ __init__.py
  â”‚   â”œâ”€â”€ missing.py       # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Phase 1)
  â”‚   â”œâ”€â”€ outliers.py      # ì´ìƒì¹˜ ì²˜ë¦¬
  â”‚   â”œâ”€â”€ scaling.py       # ìŠ¤ì¼€ì¼ë§
  â”‚   â”œâ”€â”€ reduction.py     # ì°¨ì› ì¶•ì†Œ (Phase 2)
  â”‚   â”œâ”€â”€ regime.py        # ë ˆì§ ë¶„ì„
  â”‚   â””â”€â”€ events.py        # ì´ë²¤íŠ¸ ì²˜ë¦¬ (Phase 3)
  â”œâ”€â”€ cv.py
  â””â”€â”€ utils.py
```

**ì¥ì **:
- ê¸°ëŠ¥ë³„ ë¶„ë¦¬ë¡œ ìœ ì§€ë³´ìˆ˜ ìš©ì´
- í…ŒìŠ¤íŠ¸ ì‘ì„± ì‰¬ì›€
- ì„ íƒì  ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥

---

## ğŸ§ª ê²€ì¦ ê³„íš

### 1. Unit Tests

ê° ê¸°ëŠ¥ë³„ë¡œ í…ŒìŠ¤íŠ¸ ì‘ì„±:

```python
# tests/test_preprocessing_missing.py
def test_missing_mask_creation():
    """ê²°ì¸¡ ë§ˆìŠ¤í¬ê°€ ì˜¬ë°”ë¥´ê²Œ ìƒì„±ë˜ëŠ”ì§€ í™•ì¸"""
    pass

def test_locf_no_future_leakage():
    """LOCFê°€ ë¯¸ë˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸"""
    pass

# tests/test_preprocessing_reduction.py
def test_pca_preserves_variance():
    """PCAê°€ ì§€ì •ëœ ë¶„ì‚°ì„ ë³´ì¡´í•˜ëŠ”ì§€ í™•ì¸"""
    pass
```

### 2. Integration Tests

ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸:

```python
# tests/test_pipeline_integration.py
def test_full_preprocessing_pipeline():
    """ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì´ ìˆœì„œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸"""
    
    loader = DataLoader()
    train_df, _ = loader.load_data()
    
    # Phase 1
    train_df = loader.create_missing_masks(train_df)
    train_df = loader.handle_missing_values(train_df, strategy='safe')
    
    # Phase 2
    clusters = loader.detect_feature_clusters(train_df)
    train_df = loader.reduce_feature_redundancy(train_df, clusters)
    
    # Validate
    assert train_df.isna().sum().sum() == 0, "No missing values should remain"
    # ... more assertions ...
```

### 3. Backtest Validation

OOF ì„±ëŠ¥ìœ¼ë¡œ ê²€ì¦:

```python
# scripts/validate_preprocessing.py
"""
ê° Phaseì˜ ì „ì²˜ë¦¬ë¥¼ ì ìš©í•œ í›„ OOF Sharpe Ratio ë¹„êµ:
- Baseline (í˜„ì¬ ì½”ë“œ)
- Phase 1 (ë°œí‘œì¼ ì •ë ¬ + ê²°ì¸¡ ë§ˆìŠ¤í¬ + LOCF)
- Phase 2 (+ PCA + ë ˆì§ ê°€ì¤‘ì¹˜)
- Phase 3 (+ ì´ë²¤íŠ¸ ë”ë¯¸)
"""
```

---

## ğŸ“… íƒ€ì„ë¼ì¸

| Phase | ê¸°ëŠ¥ | ì˜ˆìƒ ì‹œê°„ | ë‹´ë‹¹ |
|-------|------|-----------|------|
| **Phase 1** | ë°œí‘œì¼ ì •ë ¬ | 2ì¼ | - |
| | ê²°ì¸¡ ë§ˆìŠ¤í¬ | 1ì¼ | - |
| | LOCF ê°•í™” | 1ì¼ | - |
| | **ì†Œê³„** | **4ì¼** | |
| **Phase 2** | ìƒê´€ í´ëŸ¬ìŠ¤í„°ë§ | 2ì¼ | - |
| | ê·¸ë£¹ PCA | 2ì¼ | - |
| | ë ˆì§ ê°€ì¤‘ì¹˜ | 1ì¼ | - |
| | **ì†Œê³„** | **5ì¼** | |
| **Phase 3** | ì´ë²¤íŠ¸ ë”ë¯¸ | 2ì¼ | - |
| | Granger ë¶„ì„ | 2ì¼ | - |
| | **ì†Œê³„** | **4ì¼** | |
| **ì´í•©** | | **13ì¼ (ì•½ 2.5ì£¼)** | |

---

## ğŸ“ í•™ìŠµ í¬ì¸íŠ¸

### ê¸ˆìœµ ì‹œê³„ì—´ íŠ¹í™” ì§€ì‹

1. **ë°œí‘œì¼ ì •ë ¬ì˜ ì¤‘ìš”ì„±**
   - CPIëŠ” ë§¤ì›” ì¤‘ìˆœ ë°œí‘œë˜ì§€ë§Œ, "2ì›” CPI"ëŠ” 3ì›” 15ì¼ì— ë°œí‘œë¨
   - 2ì›” 28ì¼ì— 2ì›” CPIë¥¼ ì‚¬ìš©í•˜ë©´ **ë¯¸ë˜ì •ë³´ ìœ ì¶œ**!

2. **ê²°ì¸¡ì˜ ì˜ë¯¸**
   - ì¼ë°˜ ML: ê²°ì¸¡ = ë…¸ì´ì¦ˆ â†’ ì œê±°/ë³´ê°„
   - ê¸ˆìœµ: ê²°ì¸¡ = ì‹ í˜¸ â†’ "ê±°ë˜ ì¤‘ë‹¨", "ë°ì´í„° ë¯¸ê³µê°œ" ë“±ì˜ ì •ë³´

3. **ë³€ë™ì„± ë ˆì§**
   - 2008 ê¸ˆìœµìœ„ê¸°: ê³ ë³€ë™ì„± â†’ ë…¸ì´ì¦ˆ ë§ìŒ â†’ ë‚®ì€ ê°€ì¤‘ì¹˜
   - 2010-2019: ì €ë³€ë™ì„± â†’ ì‹ í˜¸ ëª…í™• â†’ ë†’ì€ ê°€ì¤‘ì¹˜

4. **ê³µì„ ì„±ì˜ ìœ„í—˜**
   - VIX, ATR, Realized Volì´ ëª¨ë‘ 0.95 ìƒê´€
   - íŠ¸ë¦¬ ëª¨ë¸: ê°™ì€ splitì„ ì—¬ëŸ¬ ë²ˆ â†’ ê³¼ì í•©
   - ì„ í˜• ëª¨ë¸: ê³„ìˆ˜ ë¶ˆì•ˆì • â†’ í•´ì„ ë¶ˆê°€

---

## ğŸ”— ì°¸ê³  ìë£Œ

- **Advances in Financial Machine Learning** (Marcos LÃ³pez de Prado)
  - Chapter 3: Labeling
  - Chapter 4: Sample Weights
  - Chapter 5: Fractional Differentiation

- **Machine Learning for Asset Managers** (Marcos LÃ³pez de Prado)
  - Chapter 2: Denoising and Detoning

- **Quantitative Trading** (Ernest Chan)
  - Chapter 2: Mean Reversion
  - Chapter 5: Risk Management

---

## âœ… Action Items

- [ ] **Week 1**: Phase 1 êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
  - [ ] `create_missing_masks()` êµ¬í˜„
  - [ ] `align_announcement_dates()` êµ¬í˜„ (ê°„ë‹¨ ë²„ì „)
  - [ ] `handle_missing_values()` LOCF ê°•í™”
  - [ ] Unit tests ì‘ì„±
  
- [ ] **Week 2**: Phase 2 êµ¬í˜„ ë° ê²€ì¦
  - [ ] `detect_feature_clusters()` êµ¬í˜„
  - [ ] `apply_group_pca()` êµ¬í˜„
  - [ ] `calculate_regime_weights()` êµ¬í˜„
  - [ ] OOF Sharpe ë¹„êµ ì‹¤í—˜
  
- [ ] **Week 3**: Phase 3 ë° í†µí•©
  - [ ] `add_event_dummies()` êµ¬í˜„
  - [ ] ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•©
  - [ ] ë…¸íŠ¸ë¶ ì—…ë°ì´íŠ¸ (EDAì— ìƒˆ ê¸°ëŠ¥ ì¶”ê°€)
  - [ ] ë¬¸ì„œí™” ì™„ë£Œ

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-11-12  
**ì‘ì„±ì**: AI Assistant  
**ìƒíƒœ**: ğŸ“ Draft (ê²€í†  í•„ìš”)
