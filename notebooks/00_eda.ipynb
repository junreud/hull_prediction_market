{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ğŸ“Š Exploratory Data Analysis (EDA)\n",
    "## Hull Tactical Market Prediction - Phase 1\n",
    "\n",
    "**Date:** 2025-11-11\n",
    "\n",
    "**Objectives:**\n",
    "1. Feature Group Analysis (M/E/I/P/V/S/D)\n",
    "2. Target Variable Time Series Characteristics  \n",
    "3. Correlation Analysis\n",
    "4. Market Regime Identification\n",
    "5. Train/Test Distribution Shift Detection\n",
    "6. Feature Stationarity Assessment\n",
    "7. Benchmark Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get project root (go up one level from notebooks folder)\n",
    "notebook_dir = os.getcwd() if '__file__' not in dir() else os.path.dirname(os.path.abspath(__file__))\n",
    "project_root = os.path.dirname(notebook_dir) if 'notebooks' in notebook_dir else notebook_dir\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical tests\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Project utilities\n",
    "from src.data import DataLoader, calculate_benchmark_score\n",
    "from src.utils import set_seed, load_config\n",
    "\n",
    "# Load configuration (using absolute path from project root)\n",
    "config_path = os.path.join(project_root, 'conf', 'params.yaml')\n",
    "config = load_config(config_path)\n",
    "print(f\"ğŸ“‹ Loaded config from: {config_path}\")\n",
    "print(f\"ğŸ“‹ Seed: {config['seed']}\")\n",
    "\n",
    "# Set random seed from config\n",
    "set_seed(config['seed'])\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Plot settings  \n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print('âœ… Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to project root\n",
    "os.chdir(project_root)\n",
    "print(f\"ğŸ“‚ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Initialize DataLoader (now it can find conf/params.yaml)\n",
    "loader = DataLoader()\n",
    "\n",
    "# Load data\n",
    "train_df, test_df = loader.load_data()\n",
    "\n",
    "print(f'ğŸ“¦ Train: {train_df.shape}, Test: {test_df.shape}')\n",
    "print(f'ğŸ“… Date range: {train_df[\"date_id\"].min()} - {train_df[\"date_id\"].max()}')\n",
    "\n",
    "# Display feature groups\n",
    "print('\\nğŸ·ï¸ Feature Groups:')\n",
    "for group, features in loader.feature_groups.items():\n",
    "    print(f'  {group}: {len(features)} features')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value comprehensive analysis\n",
    "print(\"=\"*60)\n",
    "print(\"MISSING VALUE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "loader = DataLoader()\n",
    "train_df, _ = loader.load_data()\n",
    "# 1. Get missing summary from DataLoader\n",
    "missing_summary = loader.get_missing_summary(train_df)\n",
    "print(\"\\n1. Missing Summary (first 20 features):\")\n",
    "print(missing_summary.head(20))\n",
    "\n",
    "# 2. Check for intermittent missing values\n",
    "print(\"\\n2. Intermittent Missing Detection:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "features_with_intermittent = []\n",
    "features_initial_only = []\n",
    "\n",
    "for col in train_df.columns:\n",
    "    if col in ['date_id', 'is_scored', 'forward_returns']:\n",
    "        continue\n",
    "    \n",
    "    series = train_df[col]\n",
    "    \n",
    "    # Find first valid index : ì‹œë¦¬ì¦ˆì—ì„œ ì²« ë²ˆì§¸ë¡œ 'ìœ íš¨í•œ ê°’(= NaNì´ ì•„ë‹Œ ê°’)'ì´ ë“±ì¥í•˜ëŠ” index(í–‰ ë²ˆí˜¸)ë¥¼ ì°¾ëŠ” í•¨ìˆ˜\n",
    "    first_valid = series.first_valid_index()\n",
    "    \n",
    "    if first_valid is None:\n",
    "        continue  # Completely missing column\n",
    "    \n",
    "    # Check if there are any NaN values AFTER first valid index\n",
    "    after_first_valid = series.loc[first_valid:]\n",
    "    has_intermittent = after_first_valid.isna().any() # ê²°ì¸¡ì¹˜ê°€ ì¡´ì¬í•˜ë©´ True, ì—†ìœ¼ë©´ False\n",
    "    \n",
    "    if has_intermittent:\n",
    "        features_with_intermittent.append(col)\n",
    "    else:\n",
    "        features_initial_only.append(col)\n",
    "\n",
    "print(f\"Features with intermittent missing: {len(features_with_intermittent)}\")\n",
    "print(f\"Features with initial-only missing: {len(features_initial_only)}\")\n",
    "\n",
    "if features_with_intermittent:\n",
    "    print(f\"\\nâš ï¸ Features with intermittent gaps (first 10):\")\n",
    "    for feat in features_with_intermittent[:10]:\n",
    "        print(f\"  - {feat}\")\n",
    "else:\n",
    "    print(\"\\nâœ… No intermittent missing values found - all missing is initial only!\")\n",
    "\n",
    "# 3. Visualize missing patterns for top features\n",
    "print(\"\\n3. Missing Pattern Visualization (top 5 features by missing %):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "top_missing = missing_summary.nlargest(5, 'missing_pct') # nlargest : missing_pct ì¹¼ëŸ¼ì—ì„œ ê°’ì´ ê°€ì¥ í° nê°œì˜ í–‰ì„ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ë°˜í™˜.\n",
    "\n",
    "fig, axes = plt.subplots(len(top_missing), 1, figsize=(16, 3*len(top_missing)))\n",
    "if len(top_missing) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (_, row) in enumerate(top_missing.iterrows()):\n",
    "    feat = row['feature']\n",
    "    series = train_df[feat]\n",
    "    \n",
    "    # Create binary mask (1=valid, 0=missing)\n",
    "    # ~series.isna() : ê°’ ì¡´ì¬ ì—¬ë¶€, astype(int) : bool ê°’ì„ intí˜•ìœ¼ë¡œ ë³€í™˜\n",
    "    mask = (~series.isna()).astype(int)\n",
    "    \n",
    "    axes[idx].plot(train_df['date_id'], mask, linewidth=0.5)\n",
    "    axes[idx].set_title(f\"{feat} - {row['missing_pct']:.1f}% missing\")\n",
    "    axes[idx].set_ylabel('Valid (1) / Missing (0)')\n",
    "    axes[idx].set_ylim(-0.1, 1.1)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark first and last valid indices\n",
    "    if row['first_valid_idx'] >= 0:\n",
    "        first_date = train_df.loc[row['first_valid_idx'], 'date_id']\n",
    "        axes[idx].axvline(x=first_date, color='green', linestyle='--', \n",
    "                         alpha=0.5, label=f'First valid: {first_date}')\n",
    "    if row['last_valid_idx'] >= 0:\n",
    "        last_date = train_df.loc[row['last_valid_idx'], 'date_id']\n",
    "        axes[idx].axvline(x=last_date, color='red', linestyle='--', \n",
    "                         alpha=0.5, label=f'Last valid: {last_date}')\n",
    "    axes[idx].legend(loc='upper left')\n",
    "\n",
    "axes[-1].set_xlabel('Date ID')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ë§ì€ ì¹¼ëŸ¼ 5ê°œ ì‹œê°í™”.\n",
    "print(\"\\nâœ… Missing value analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. Feature Group Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each feature group\n",
    "for group, features in loader.feature_groups.items():\n",
    "    if not features or group == 'D':\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\\n{group} Features ({len(features)})\\n{'='*60}\")\n",
    "    \n",
    "    # Descriptive statistics : count, mean, std, min, 25%, 50%, 75%, max\n",
    "    desc = train_df[features].describe()\n",
    "    print(desc)\n",
    "    \n",
    "    # Plot sample distributions\n",
    "    sample_features = features[:min(4, len(features))]\n",
    "    fig, axes = plt.subplots(1, len(sample_features), figsize=(16, 4))\n",
    "    if len(sample_features) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, feat in enumerate(sample_features):\n",
    "        train_df[feat].hist(bins=50, ax=axes[idx], alpha=0.7) # í•˜ë‚˜ì˜ ì¹¼ëŸ¼ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ 50ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆˆ ê·¸ë˜í”„\n",
    "        axes[idx].set_title(f'{feat} Distribution')\n",
    "    \n",
    "    plt.suptitle(f'{group} Group - Sample Distributions')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 5. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'forward_returns'  # S&P500ì˜ ë‹¤ìŒë‚  ìˆ˜ìµë¥ \n",
    "\n",
    "# Time series plot\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(train_df['date_id'], train_df[target], alpha=0.6, linewidth=0.5)\n",
    "plt.title('Forward Returns Over Time')\n",
    "plt.xlabel('Date ID')\n",
    "plt.ylabel('Forward Returns')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Distribution plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(train_df[target].dropna(), bins=100, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Distribution')\n",
    "axes[0].set_xlabel('Forward Returns')\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(train_df[target].dropna(), dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title('Q-Q Plot')\n",
    "\n",
    "# Box plot\n",
    "axes[2].boxplot(train_df[target].dropna())\n",
    "axes[2].set_title('Box Plot')\n",
    "axes[2].set_ylabel('Forward Returns')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(f\"Mean: {train_df[target].mean():.6f}\")\n",
    "print(f\"Std: {train_df[target].std():.6f}\")\n",
    "print(f\"Skewness: {stats.skew(train_df[target].dropna()):.4f}\")\n",
    "print(f\"Kurtosis: {stats.kurtosis(train_df[target].dropna()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 6. Autocorrelation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê³„ì—´ ë°ì´í„°(íšŒê·€)ì˜ ì„±ì§ˆì„ ë¶„ì„í•˜ëŠ”ë° í™œìš©ë˜ëŠ” ìƒê´€ë„í‘œ(Correlogram) : ACF and PACF plots\n",
    "# xê°€ 0ì¼ë•ŒëŠ” yëŠ” ë¬´ì¡°ê±´ 1, ê·¸ ì™¸ xê°’ì¸ê²½ìš°, yëŠ” 0ì´ë‘ ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ë‹¤. \n",
    "# -> 0ì´ë‘ ê°€ê¹Œìš¸ ìˆ˜ë¡ ë…ë¦½ì . (íŒŒë€ì˜ì—­ ì•ˆì— ë“¤ì–´ì™€ì•¼ ì¢‹ìŒ.)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# ACF (Autocorrelation Function, ìê¸° ìƒê´€ í•¨ìˆ˜) : ì „ì²´ ìƒê´€ ì¸¡ì •\n",
    "plot_acf(train_df[target].dropna(), lags=50, ax=axes[0])\n",
    "axes[0].set_title('Autocorrelation Function (ACF)')\n",
    "\n",
    "# PSCF (Partial Autocorrelation Function, í¸ìê¸°ìƒê´€í•¨ìˆ˜) : ê°œë³„ ì‹œì°¨ë³„ ìê¸° ìƒê´€ ì¸¡ì •\n",
    "plot_pacf(train_df[target].dropna(), lags=50, ax=axes[1])\n",
    "axes[1].set_title('Partial Autocorrelation Function (PACF)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print significant lags\n",
    "acf_vals = acf(train_df[target].dropna(), nlags=20)\n",
    "print(\"Significant ACF lags (|r| > 0.05):\")\n",
    "for i, val in enumerate(acf_vals[1:], 1):\n",
    "    if abs(val) > 0.05:\n",
    "        print(f\"  Lag {i}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample features for correlation\n",
    "sample_features = []\n",
    "for group, features in loader.feature_groups.items():\n",
    "    if group != 'D' and features:\n",
    "        sample_features.extend(features[:min(5, len(features))])\n",
    "\n",
    "sample_features.append(target)\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = train_df[sample_features].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, vmin=-1, vmax=1)\n",
    "plt.title('Feature Correlation Heatmap (Sample)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top correlations with target\n",
    "target_corr = corr_matrix[target].drop(target).sort_values(ascending=False)\n",
    "print(\"\\nTop 10 Positive Correlations with Target:\")\n",
    "print(target_corr.head(10))\n",
    "print(\"\\nTop 10 Negative Correlations with Target:\")\n",
    "print(target_corr.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 8. Market Regime Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "í˜„ì¬ ë…¸íŠ¸ë¶ì—ì„œëŠ” forward_returns ì—´ê³¼ ë‹¤ë¥¸ í”¼ì²˜ ê·¸ë£¹(ì˜ˆ: Vë¡œ ì‹œì‘í•˜ëŠ” ì—´ë“¤)ì— ëŒ€í•´ ë¡¤ë§ ë³€ë™ì„±ì„ ê³„ì‚°í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ë•Œ ìœˆë„ìš° í¬ê¸°ëŠ” ì„¤ì • íŒŒì¼(config['model_risk']['risk_window'])ì—ì„œ ì •ì˜ëœ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë¡¤ë§ ë³€ë™ì„±ì€ ë°ì´í„°ì˜ ê³ ë³€ë™(high volatility) ë˜ëŠ” ì €ë³€ë™(low volatility) êµ¬ê°„ì„ ì‹ë³„í•˜ëŠ” ë° ìœ ìš©í•˜ë©°, ì‹œì¥ ìƒíƒœ ë¶„ì„ì´ë‚˜ ë¦¬ìŠ¤í¬ í‰ê°€ì— ìì£¼ í™œìš©ë©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling volatility (using config window)\n",
    "window = config['model_risk']['risk_window']\n",
    "train_df['rolling_vol'] = train_df[target].rolling(window).std()\n",
    "\n",
    "# Identify regimes (quartiles)\n",
    "vol_quartiles = train_df['rolling_vol'].quantile([0.25, 0.5, 0.75])\n",
    "print(f\"Volatility Quartiles (window={window}):\\n{vol_quartiles}\")\n",
    "\n",
    "# Classify regimes\n",
    "def classify_regime(vol):\n",
    "    if pd.isna(vol):\n",
    "        return 'Unknown'\n",
    "    elif vol < vol_quartiles[0.25]:\n",
    "        return 'Low Vol'\n",
    "    elif vol < vol_quartiles[0.75]:\n",
    "        return 'Medium Vol'\n",
    "    else:\n",
    "        return 'High Vol'\n",
    "\n",
    "train_df['regime'] = train_df['rolling_vol'].apply(classify_regime)\n",
    "\n",
    "# Plot regimes\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10), sharex=True)\n",
    "\n",
    "# Returns with regime coloring\n",
    "for regime in ['Low Vol', 'Medium Vol', 'High Vol']:\n",
    "    mask = train_df['regime'] == regime\n",
    "    axes[0].scatter(train_df.loc[mask, 'date_id'], \n",
    "                   train_df.loc[mask, target],\n",
    "                   label=regime, alpha=0.5, s=1)\n",
    "axes[0].set_ylabel('Forward Returns')\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Returns by Volatility Regime')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Rolling volatility\n",
    "axes[1].plot(train_df['date_id'], train_df['rolling_vol'], linewidth=1)\n",
    "axes[1].axhline(vol_quartiles[0.25], color='g', linestyle='--', label='Q1')\n",
    "axes[1].axhline(vol_quartiles[0.75], color='r', linestyle='--', label='Q3')\n",
    "axes[1].set_xlabel('Date ID')\n",
    "axes[1].set_ylabel('Rolling Volatility')\n",
    "axes[1].legend()\n",
    "axes[1].set_title(f'Rolling Volatility ({window}-day window)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance by regime\n",
    "print(\"\\nPerformance by Regime:\")\n",
    "regime_stats = train_df.groupby('regime')[target].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "print(regime_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare rolling volatility of V columns with forward_returns\n",
    "v_columns = [col for col in train_df.columns if col.startswith('V')]\n",
    "rolling_vol_v = train_df[v_columns].rolling(window).std()\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(len(v_columns) + 1, 1, figsize=(16, 5 * (len(v_columns) + 1)), sharex=True)\n",
    "\n",
    "# Rolling volatility of forward_returns\n",
    "axes[0].plot(train_df['date_id'], train_df['rolling_vol'], linewidth=1, label='Forward Returns Volatility')\n",
    "axes[0].set_title('Rolling Volatility Comparison')\n",
    "axes[0].set_ylabel('Volatility')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Rolling volatility of V columns\n",
    "for idx, col in enumerate(v_columns):\n",
    "    axes[idx + 1].plot(train_df['date_id'], rolling_vol_v[col], linewidth=1, label=f'{col} Volatility')\n",
    "    axes[idx + 1].set_ylabel('Volatility')\n",
    "    axes[idx + 1].legend()\n",
    "    axes[idx + 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel('Date ID')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 9. Stationarity Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "ì´ ì…€ì€ Augmented Dickey-Fuller (ADF) í…ŒìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • í”¼ì²˜(feature)ë“¤ì´ \n",
    "**ì •ìƒì„±(stationarity)**ì„ ê°€ì§€ëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤. ì •ìƒì„±ì€ ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ì—ì„œ ì¤‘ìš”í•œ ê°œë…ìœ¼ë¡œ, \n",
    "ë°ì´í„°ì˜ í‰ê· ê³¼ ë¶„ì‚°ì´ ì‹œê°„ì— ë”°ë¼ ì¼ì •í•œì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì•„ë˜ëŠ” ì½”ë“œì˜ ì£¼ìš” ë¶€ë¶„ê³¼ ê·¸ ì˜ë¯¸ë¥¼ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test stationarity for target and sample features\n",
    "test_features = [target] + [f for g, feats in loader.feature_groups.items() \n",
    "                            for f in feats[:2] if g != 'D' and feats][:10] # ì „ì²´ ë‹¤ í•˜ë ¤ë©´ [:2], [:10] ì œê±°\n",
    "\n",
    "stationarity_results = []\n",
    "\n",
    "for feat in test_features:\n",
    "    series = train_df[feat].dropna()\n",
    "    \n",
    "    if len(series) < 50:\n",
    "        continue\n",
    "    \n",
    "    # ADF test\n",
    "    '''\n",
    "    * ê° í”¼ì²˜ì— ëŒ€í•´ ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:\n",
    "    1. ê²°ì¸¡ê°’ ì œê±°: dropna()ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²°ì¸¡ê°’ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "    2. ìƒ˜í”Œ í¬ê¸° í™•ì¸: ë°ì´í„°ê°€ 50ê°œ ë¯¸ë§Œì¸ ê²½ìš° í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\n",
    "    3. ADF í…ŒìŠ¤íŠ¸ ì‹¤í–‰: adfuller í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ADF í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "    4. adfullerëŠ” ì •ìƒì„±ì„ í‰ê°€í•˜ëŠ” í†µê³„ëŸ‰ê³¼ p-valueë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    5. ê²°ê³¼ ì €ì¥: í”¼ì²˜ ì´ë¦„, ADF í†µê³„ëŸ‰, p-value, ì •ìƒì„± ì—¬ë¶€ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    '''\n",
    "    adf_result = adfuller(series, autolag='AIC')\n",
    "    \n",
    "    stationarity_results.append({\n",
    "        'Feature': feat,\n",
    "        'ADF Statistic': adf_result[0],\n",
    "        'ADF p-value': adf_result[1],\n",
    "        'Stationary (p<0.05)': adf_result[1] < 0.05\n",
    "    })\n",
    "\n",
    "stationarity_df = pd.DataFrame(stationarity_results)\n",
    "print(\"Stationarity Test Results (ADF):\")\n",
    "print(stationarity_df.sort_values('ADF p-value'))\n",
    "\n",
    "# Count stationary vs non-stationary\n",
    "n_stationary = stationarity_df['Stationary (p<0.05)'].sum()\n",
    "n_total = len(stationarity_df)\n",
    "print(f\"\\nStationary: {n_stationary}/{n_total} ({n_stationary/n_total*100:.1f}%)\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 10. Train/Test Distribution Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "ì´ ì…€ì€ Kolmogorov-Smirnov (KS) í…ŒìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ë°ì´í„°(train_df)ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°(test_df) ê°„ì˜ íŠ¹ì§• ë¶„í¬ ì°¨ì´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. KS í…ŒìŠ¤íŠ¸ëŠ” ë‘ ë°ì´í„° ë¶„í¬ê°€ ë™ì¼í•œì§€ ì—¬ë¶€ë¥¼ í‰ê°€í•˜ëŠ” ë¹„ëª¨ìˆ˜ì  í†µê³„ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” ì½”ë“œì˜ ì£¼ìš” ë¶€ë¶„ê³¼ ê·¸ ì˜ë¯¸ë¥¼ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions for common features\n",
    "common_features = [c for c in train_df.columns if c in test_df.columns \n",
    "                   and c not in ['date_id', 'is_scored']]\n",
    "\n",
    "# Sample features to test\n",
    "test_sample = [f for f in common_features if not f.startswith('lagged')][:15]\n",
    "\n",
    "shift_results = []\n",
    "\n",
    "for feat in test_sample:\n",
    "    train_vals = train_df[feat].dropna()\n",
    "    test_vals = test_df[feat].dropna()\n",
    "    \n",
    "    if len(train_vals) < 10 or len(test_vals) < 5:\n",
    "        continue\n",
    "    \n",
    "    # KS test\n",
    "    ks_stat, ks_pval = ks_2samp(train_vals, test_vals)\n",
    "    \n",
    "    shift_results.append({\n",
    "        'Feature': feat,\n",
    "        'KS Statistic': ks_stat,\n",
    "        'KS p-value': ks_pval,\n",
    "        'Significant Shift (p<0.05)': ks_pval < 0.05\n",
    "    })\n",
    "\n",
    "shift_df = pd.DataFrame(shift_results)\n",
    "print(\"Distribution Shift Analysis (KS Test):\")\n",
    "print(shift_df.sort_values('KS Statistic', ascending=False))\n",
    "\n",
    "# Count shifts\n",
    "n_shifts = shift_df['Significant Shift (p<0.05)'].sum()\n",
    "print(f\"\\nFeatures with significant shift: {n_shifts}/{len(shift_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 11. Benchmark Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate benchmark with allocation=1.0\n",
    "score, returns, metrics = calculate_benchmark_score(train_df, allocation=1.0)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BENCHMARK METRICS (Allocation = 1.0)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nScore: {score:.4f}\")\n",
    "print(f\"Sharpe Ratio: {metrics['sharpe']:.4f}\")\n",
    "print(f\"Annual Sharpe: {metrics['annual_sharpe']:.4f}\")\n",
    "print(f\"Volatility Penalty: {metrics['vol_penalty']:.4f}\")\n",
    "print(f\"\\nAnnual Return: {metrics['annual_return']:.2%}\")\n",
    "print(f\"Annual Volatility: {metrics['annual_vol']:.2%}\")\n",
    "print(f\"Max Drawdown: {metrics['max_drawdown']:.2%}\")\n",
    "print(f\"Win Rate: {metrics['win_rate']:.2%}\")\n",
    "\n",
    "# Cumulative returns plot\n",
    "cumulative_returns = (1 + returns).cumprod()\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Cumulative returns\n",
    "axes[0].plot(cumulative_returns.values, linewidth=1.5)\n",
    "axes[0].set_title('Benchmark Strategy - Cumulative Returns (Allocation=1.0)')\n",
    "axes[0].set_ylabel('Cumulative Return')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=1.0, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Drawdown\n",
    "running_max = cumulative_returns.expanding().max()\n",
    "drawdown = (cumulative_returns - running_max) / running_max\n",
    "\n",
    "axes[1].fill_between(range(len(drawdown)), drawdown.values, 0, alpha=0.3, color='red')\n",
    "axes[1].plot(drawdown.values, linewidth=1, color='darkred')\n",
    "axes[1].set_title('Drawdown')\n",
    "axes[1].set_xlabel('Days')\n",
    "axes[1].set_ylabel('Drawdown')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## 12. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EDA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nğŸ“¦ Data Shape:\")\n",
    "print(f\"  Train: {train_df.shape}\")\n",
    "print(f\"  Test: {test_df.shape}\")\n",
    "\n",
    "print(f\"\\nğŸ·ï¸ Feature Groups:\")\n",
    "for group, features in loader.feature_groups.items():\n",
    "    print(f\"  {group}: {len(features)} features\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Target Statistics:\")\n",
    "print(f\"  Mean: {train_df[target].mean():.6f}\")\n",
    "print(f\"  Std: {train_df[target].std():.6f}\")\n",
    "print(f\"  Sharpe: {train_df[target].mean()/train_df[target].std():.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Benchmark Performance:\")\n",
    "print(f\"  Score: {score:.4f}\")\n",
    "print(f\"  Annual Sharpe: {metrics['annual_sharpe']:.4f}\")\n",
    "print(f\"  Max Drawdown: {metrics['max_drawdown']:.2%}\")\n",
    "\n",
    "print(f\"\\nâš ï¸ Key Findings:\")\n",
    "print(f\"  - Stationary features: {n_stationary}/{n_total}\")\n",
    "print(f\"  - Distribution shifts: {n_shifts}/{len(shift_df)}\")\n",
    "print(f\"  - Missing data: See quality check above\")\n",
    "\n",
    "print(\"\\nâœ… EDA Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
