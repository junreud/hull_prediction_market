{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train csv data\n",
    "df = pd.read_csv(\"../data/raw/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 칼럼명을 test 칼럼명과 동일하게 맞추기.\n",
    "df[\"lagged_risk_free_rate\"] = df[\"risk_free_rate\"]#.shift(1)\n",
    "df[\"lagged_market_forward_excess_returns\"] = df[\"market_forward_excess_returns\"]#.shift(1)\n",
    "\n",
    "# x, y split\n",
    "X = df.drop(columns=['forward_returns', 'risk_free_rate', 'market_forward_excess_returns'])\n",
    "y = df['forward_returns']\n",
    "\n",
    "\n",
    "X.shape, y.shape, X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname('src'))))\n",
    "from src.cv import PurgedWalkForwardCV\n",
    "\n",
    "cv_split = PurgedWalkForwardCV(n_splits=5, \n",
    "                         embargo=20, \n",
    "                         purge=True,\n",
    "                         purge_period=5,\n",
    "                         train_ratio=0.8)\n",
    "\n",
    "split_df = []\n",
    "for train_idx, val_idx in cv_split.split(X, y):\n",
    "    print(train_idx, val_idx)\n",
    "    \n",
    "    X_train = X.iloc[train_idx]\n",
    "    y_train = y.iloc[train_idx]\n",
    "\n",
    "    X_valid = X.iloc[val_idx]\n",
    "    y_valid = y.iloc[val_idx]\n",
    "\n",
    "    split_df.append((X_train, y_train, X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_config\n",
    "\n",
    "config_path = \"../conf/params.yaml\"\n",
    "config = load_config(config_path)\n",
    "\n",
    "return_cfg = config.get(\"model_return\", {}).get(\"lightgbm\", {})\n",
    "risk_cfg = config.get(\"risk\", {}).get(\"lightgbm\", {}).get(\"fixed_params\", {})\n",
    "return_model_params = {\n",
    "    \"n_estimators\": return_cfg.get(\"n_estimators\", 800),\n",
    "    \"learning_rate\": return_cfg.get(\"learning_rate\", 0.05),\n",
    "    \"num_leaves\": return_cfg.get(\"num_leaves\", 64),\n",
    "    \"feature_fraction\": return_cfg.get(\"feature_fraction\", 0.8),\n",
    "    \"bagging_fraction\": return_cfg.get(\"bagging_fraction\", 0.8),\n",
    "    \"bagging_freq\": return_cfg.get(\"bagging_freq\", 5),\n",
    "    \"min_child_samples\": return_cfg.get(\"min_child_samples\", 20),\n",
    "    \"max_depth\": return_cfg.get(\"max_depth\", -1),\n",
    "    \"objective\": return_cfg.get(\"objective\", \"regression\"),\n",
    "    \"random_state\": return_cfg.get(\"random_state\", 42),\n",
    "    \"verbosity\": return_cfg.get(\"verbosity\", -1),\n",
    "    \"n_jobs\": -1\n",
    "}\n",
    "risk_model_params = {\n",
    "    \"n_estimators\": return_cfg.get(\"n_estimators\", 600),\n",
    "    \"learning_rate\": return_cfg.get(\"learning_rate\", 0.05),\n",
    "    \"num_leaves\": risk_cfg.get(\"num_leaves\", 48),\n",
    "    \"feature_fraction\": 0.7,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"min_child_samples\": risk_cfg.get(\"min_data_in_leaf\", 15),\n",
    "    \"reg_alpha\": risk_cfg.get(\"lambda_l1\", 0.1),\n",
    "    \"reg_lambda\": risk_cfg.get(\"lambda_l2\", 0.1),\n",
    "    \"objective\": \"regression\",\n",
    "    \"random_state\": risk_cfg.get(\"random_state\", 42),\n",
    "    \"verbosity\": risk_cfg.get(\"verbosity\", -1),\n",
    "    \"n_jobs\": -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "import numpy as np\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname('src'))))\n",
    "from src.metric import CompetitionMetric\n",
    "from src.position import PositionOptimizer, create_position_mapper\n",
    "from src.risk import RiskLabeler\n",
    "from pathlib import Path\n",
    "\n",
    "risk_labeler = RiskLabeler(window=config.get(\"risk\", {}).get(\"label\", {}).get(\"window\", 20), config_path=config_path)\n",
    "metric_calc = CompetitionMetric()\n",
    "fold_results = []\n",
    "fold_params = []\n",
    "return_model_paths = []\n",
    "risk_model_paths = []\n",
    "\n",
    "artifacts_dir = Path(\"../artifacts\")\n",
    "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for fold_idx, (X_train, y_train, X_valid, y_valid) in enumerate(split_df, start=1):\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(X_valid.shape, y_valid.shape)\n",
    "    \n",
    "    if fold_idx == 0:\n",
    "        print()\n",
    "        print(\"RETURN TRAIN COLUMNS : \", X_train.columns)\n",
    "        print()\n",
    "        \n",
    "    # 1. Train Return Model on train fold\n",
    "    # LGBMRegressor = Gradient Boosting + Leaf-wise Decision Trees + Histogram\n",
    "    return_model = LGBMRegressor(**return_model_params)\n",
    "    return_model.fit(X_train, y_train)\n",
    "    train_r_hat = return_model.predict(X_train)\n",
    "\n",
    "    # 2. Train Risk Model on train fold\n",
    "    risk_train_df = pd.concat([X_train, y_train], axis=1).sort_values(\"date_id\").copy()\n",
    "    risk_train_df[\"risk_label\"] = risk_labeler.create_labels(risk_train_df, \"forward_returns\")\n",
    "    risk_targets = risk_train_df[\"risk_label\"].bfill().ffill()\n",
    "    if risk_targets.isna().any():\n",
    "        fallback = risk_train_df[\"forward_returns\"].rolling(window=20, min_periods=5).std()\n",
    "        risk_targets = risk_targets.fillna(fallback).fillna(risk_train_df[\"forward_returns\"].abs())\n",
    "\n",
    "    if fold_idx == 0:\n",
    "        print()\n",
    "        print(\"RISK TRAIN COLUMNS : \", X_train.columns)\n",
    "        print()\n",
    "\n",
    "    risk_model = LGBMRegressor(**risk_model_params)\n",
    "    risk_model.fit(risk_train_df[X_train.columns], risk_targets)\n",
    "    train_sigma_hat = risk_model.predict(X_train)\n",
    "\n",
    "    # 3. Optimize Position Strategy on train fold\n",
    "    mapper = create_position_mapper(strategy=\"sharpe_scaling\", config_path=config_path)\n",
    "    optimizer = PositionOptimizer(mapper, config_path=config_path)\n",
    "    optimal_params = optimizer.optimize_sharpe_params(\n",
    "        r_hat=train_r_hat,\n",
    "        sigma_hat=train_sigma_hat,\n",
    "        actual_returns=y_train.values\n",
    "    )\n",
    "\n",
    "    # 4. Make predictions on validation fold\n",
    "    valid_r_hat = return_model.predict(X_valid)\n",
    "    valid_sigma_hat = risk_model.predict(X_valid)\n",
    "\n",
    "    # 5. Convert these predictions to positions\n",
    "    valid_positions = mapper.map_positions(\n",
    "        r_hat=valid_r_hat,\n",
    "        sigma_hat=valid_sigma_hat,\n",
    "        k=optimal_params[\"k\"],\n",
    "        b=optimal_params[\"b\"]\n",
    "    )\n",
    "\n",
    "    # A. fold별 모델/파라미터/feature 순서 저장 (앙상블용)\n",
    "    import joblib, json\n",
    "    fold_tag = f\"fold{fold_idx}\"\n",
    "    return_path = artifacts_dir / f\"return_model_{fold_tag}.pkl\"\n",
    "    risk_path = artifacts_dir / f\"risk_model_{fold_tag}.pkl\"\n",
    "    joblib.dump(return_model, return_path)\n",
    "    joblib.dump(risk_model, risk_path)\n",
    "    return_model_paths.append(str(return_path))\n",
    "    risk_model_paths.append(str(risk_path))\n",
    "\n",
    "    feature_cols = list(X_train.columns)\n",
    "    json.dump(feature_cols, open(artifacts_dir / \"feature_cols.json\", \"w\"))\n",
    "\n",
    "    fold_results.append({\n",
    "        \"fold\": fold_idx,\n",
    "        \"score\": None,  # placeholder until score is computed\n",
    "        \"sharpe\": None,\n",
    "        \"k\": optimal_params[\"k\"],\n",
    "        \"b\": optimal_params[\"b\"],\n",
    "    })\n",
    "    fold_params.append({\"fold\": fold_idx, **optimal_params})\n",
    "\n",
    "    # 6. Evaluate strategy Sharpe on validation fold\n",
    "    fold_score = metric_calc.calculate_score(\n",
    "        allocations=valid_positions,\n",
    "        forward_returns=y_valid.values\n",
    "    )\n",
    "\n",
    "    fold_results[-1][\"score\"] = fold_score[\"score\"]\n",
    "    fold_results[-1][\"sharpe\"] = fold_score[\"sharpe\"]\n",
    "\n",
    "    print(\n",
    "        f\"Fold {fold_idx} → score: {fold_score['score']:.4f}, \"\n",
    "        f\"sharpe: {fold_score['sharpe']:.4f}, k={optimal_params['k']:.3f}, b={optimal_params['b']:.3f}\"\n",
    "    )\n",
    "\n",
    "if fold_results:\n",
    "    mean_score = np.mean([res[\"score\"] for res in fold_results])\n",
    "    mean_sharpe = np.mean([res[\"sharpe\"] for res in fold_results])\n",
    "    print(f\"Average validation score: {mean_score:.4f}, Average sharpe: {mean_sharpe:.4f}\")\n",
    "\n",
    "results = {\n",
    "    \"fold_results\": fold_results,\n",
    "    \"mean_score\": float(np.mean([r[\"score\"] for r in fold_results])) if fold_results else 0.0,\n",
    "    \"mean_sharpe\": float(np.mean([r[\"sharpe\"] for r in fold_results])) if fold_results else 0.0,\n",
    "}\n",
    "\n",
    "# fold별 k/b 평균을 저장해 추론 시 기본값으로 사용\n",
    "if fold_params:\n",
    "    k_mean = float(np.mean([p[\"k\"] for p in fold_params]))\n",
    "    b_mean = float(np.mean([p[\"b\"] for p in fold_params]))\n",
    "    json.dump({\"k\": k_mean, \"b\": b_mean}, open(artifacts_dir / \"optimal_params.json\", \"w\"))\n",
    "\n",
    "# fold별 메타데이터 저장(앙상블 로딩용)\n",
    "ensemble_metadata = {\n",
    "    \"return_model_paths\": return_model_paths,\n",
    "    \"risk_model_paths\": risk_model_paths,\n",
    "    \"fold_params\": fold_params,\n",
    "}\n",
    "json.dump(ensemble_metadata, open(artifacts_dir / \"ensemble_metadata.json\", \"w\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hull_prediction_market",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
