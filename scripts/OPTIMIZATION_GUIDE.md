# ìˆ˜ìµë¥  ì˜ˆì¸¡ ëª¨ë¸ ìµœì í™” ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”
ì´ ë¬¸ì„œëŠ” `/src` í´ë”ì˜ ëª¨ë“  ì£¼ìš” í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ ìˆ˜ìµë¥  ì˜ˆì¸¡ ëª¨ë¸ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ¯ ìµœì í™” íŒŒì´í”„ë¼ì¸ êµ¬ì¡°

```
Step 1: ë°ì´í„° ì „ì²˜ë¦¬ (data.py)
   â†“
Step 2: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (features.py)
   â†“
Step 3: í”¼ì²˜ ì„ íƒ (features.py)
   â†“
Step 4: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (tuner.py)
   â†“
Step 5: ìµœì¢… ëª¨ë¸ í•™ìŠµ (models.py)
   â†“
Step 6: ëª¨ë¸ í•´ì„ (interpretability.py)
   â†“
Step 7: ê²°ê³¼ ì €ì¥ ë° ë¶„ì„
```

## ğŸ“‚ ì‚¬ìš©ë˜ëŠ” ì£¼ìš” ëª¨ë“ˆ ë° í•¨ìˆ˜

### 1. `src/data.py` - ë°ì´í„° ì „ì²˜ë¦¬
- **`DataLoader.load_train_data()`**: í•™ìŠµ ë°ì´í„° ë¡œë“œ
- **`DataLoader.preprocess_timeseries()`**: ì¢…í•© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
  - `add_missing_indicators()`: ê²°ì¸¡ì¹˜ íŒ¨í„´ì„ ì‹ í˜¸ë¡œ í™œìš©
  - `add_regime_indicators()`: ê¸ˆìœµ ìœ„ê¸° êµ¬ê°„ í‘œì‹œ
  - `winsorize_outliers()`: ì´ìƒì¹˜ ì²˜ë¦¬ (0.1% winsorization)
  - `normalize_features()`: ì •ê·œí™” (rank-gauss)
  - `scale_features()`: ìŠ¤ì¼€ì¼ë§ (robust scaler)

### 2. `src/features.py` - í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
- **`FeatureEngineering.fit_transform()`**: ì „ì²´ í”¼ì²˜ ìƒì„± íŒŒì´í”„ë¼ì¸
  - `create_rolling_features()`: ë¡¤ë§ í†µê³„ëŸ‰
  - `create_lag_features()`: ì‹œì°¨ í”¼ì²˜
  - `create_difference_features()`: ì°¨ë¶„ í”¼ì²˜
  - `create_interaction_features()`: ìƒí˜¸ì‘ìš© í”¼ì²˜
  - `create_technical_features()`: ê¸°ìˆ ì  ì§€í‘œ (RSI, Bollinger Bands)
  - `create_regime_features()`: ë³€ë™ì„± êµ¬ê°„ ë¶„ë¥˜
- **`select_features_by_importance()`**: ì¤‘ìš”ë„ ê¸°ë°˜ í”¼ì²˜ ì„ íƒ
- **`remove_correlated_features()`**: ì¤‘ë³µ í”¼ì²˜ ì œê±°

### 3. `src/tuner.py` - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
- **`OptunaLightGBMTuner.tune()`**: Optuna ê¸°ë°˜ ë² ì´ì§€ì•ˆ ìµœì í™”
  - TPE (Tree-structured Parzen Estimator) ìƒ˜í”ŒëŸ¬
  - Median Prunerë¡œ ì¡°ê¸° ì¢…ë£Œ
  - ì‹œê³„ì—´ êµì°¨ê²€ì¦ í†µí•©

### 4. `src/models.py` - ëª¨ë¸ í•™ìŠµ
- **`ReturnPredictor.train()`**: êµì°¨ê²€ì¦ ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ
  - ì‹œê³„ì—´ split (PurgedGroupTimeSeriesSplit)
  - OOF (Out-of-Fold) ì˜ˆì¸¡
  - í”¼ì²˜ ì¤‘ìš”ë„ ì¶”ì 
- **`ReturnPredictor.save_models()`**: ëª¨ë¸ ì €ì¥

### 5. `src/interpretability.py` - ëª¨ë¸ í•´ì„
- **`ModelInterpreter.calculate_feature_importance()`**: í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
- **`ModelInterpreter.calculate_shap_values()`**: SHAP ê°’ ê³„ì‚°
- **`ModelInterpreter.get_feature_interactions()`**: í”¼ì²˜ ìƒí˜¸ì‘ìš© ë¶„ì„
- **`ModelInterpreter.save_analysis()`**: ë¶„ì„ ê²°ê³¼ ì €ì¥

### 6. `src/cv.py` - êµì°¨ê²€ì¦ ì „ëµ
- **`PurgedGroupTimeSeriesSplit`**: ì‹œê³„ì—´ êµì°¨ê²€ì¦
  - ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ (purging)
  - ê·¸ë£¹ ê¸°ë°˜ split

### 7. `src/metric.py` - í‰ê°€ ë©”íŠ¸ë¦­
- **`CompetitionMetric.calculate_r_hat()`**: ëŒ€íšŒ ë©”íŠ¸ë¦­ ê³„ì‚°
- **`CompetitionMetric.calculate_score()`**: ì „ì²´ ìŠ¤ì½”ì–´ ê³„ì‚°

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### ë°©ë²• 1: ì „ì²´ íŒŒì´í”„ë¼ì¸ í•œ ë²ˆì— ì‹¤í–‰
```bash
cd /Users/gimjunseog/projects/kaggle/Prediction_Market
python scripts/optimize_return_model.py
```

### ë°©ë²• 2: ë‹¨ê³„ë³„ ì‹¤í–‰ (ê¶Œì¥)
```python
from scripts.optimize_return_model import ReturnModelOptimizer

# 1. Optimizer ìƒì„±
optimizer = ReturnModelOptimizer(config_path="conf/params.yaml")

# 2. Step 1: ë°ì´í„° ì „ì²˜ë¦¬
train_df, metadata = optimizer.step1_load_and_preprocess_data(
    add_missing_indicators=True,   # ê²°ì¸¡ì¹˜ ì§€í‘œ ì¶”ê°€
    add_regime_indicators=True,    # ë ˆì§ ì§€í‘œ ì¶”ê°€
    handle_outliers=True,          # ì´ìƒì¹˜ ì²˜ë¦¬
    normalize=True,                # ì •ê·œí™”
    scale=True                     # ìŠ¤ì¼€ì¼ë§
)

# 3. Step 2: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
train_engineered = optimizer.step2_feature_engineering(train_df)

# 4. Step 3: í”¼ì²˜ ì„ íƒ
train_selected, selected_features = optimizer.step3_feature_selection(
    train_engineered,
    method='correlation',      # ë˜ëŠ” 'mutual_info'
    top_n=200,                # Top 200 features
    remove_correlated=True,
    corr_threshold=0.95
)

# 5. Step 4: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
best_params = optimizer.step4_hyperparameter_tuning(
    train_selected,
    selected_features,
    n_trials=50,              # íŠœë‹ ì‹œí–‰ íšŸìˆ˜
    timeout=None              # ì‹œê°„ ì œí•œ (ì´ˆ)
)

# 6. Step 5: ìµœì¢… ëª¨ë¸ í•™ìŠµ
predictor, oof_preds, oof_score = optimizer.step5_train_final_model(
    train_selected,
    selected_features,
    best_params
)

# 7. Step 6: ëª¨ë¸ í•´ì„
interpreter = optimizer.step6_model_interpretation(
    predictor,
    train_selected,
    selected_features,
    calculate_shap=False      # SHAP ê³„ì‚° (ëŠë¦¼)
)

# 8. Step 7: ê²°ê³¼ ì €ì¥
optimizer.step7_save_results()
```

## ğŸ›ï¸ íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ

### ì „ì²˜ë¦¬ ë‹¨ê³„
```python
# ì´ìƒì¹˜ ì²˜ë¦¬ ê°•ë„ ì¡°ì ˆ
winsorize_limits=(0.001, 0.001)  # 0.1% í´ë¦¬í•‘ (ê¸°ë³¸)
winsorize_limits=(0.005, 0.005)  # 0.5% í´ë¦¬í•‘ (ë” ê°•í•˜ê²Œ)

# ì •ê·œí™” ë°©ë²•
normalize_method='rank_gauss'     # ìˆœìœ„ ê¸°ë°˜ ê°€ìš°ìŠ¤ ë³€í™˜ (ì¶”ì²œ)
normalize_method='log1p'          # Log ë³€í™˜
normalize_method='rolling_zscore' # ë¡¤ë§ Z-score

# ìŠ¤ì¼€ì¼ë§ ë°©ë²•
scale_method='robust'    # ì´ìƒì¹˜ì— ê°•í•¨ (ì¶”ì²œ)
scale_method='standard'  # í‘œì¤€ ìŠ¤ì¼€ì¼ë§
```

### í”¼ì²˜ ì„ íƒ ë‹¨ê³„
```python
# ì„ íƒ ë°©ë²•
method='correlation'   # ë¹ ë¦„, ì„ í˜• ê´€ê³„ í¬ì°©
method='mutual_info'   # ëŠë¦¼, ë¹„ì„ í˜• ê´€ê³„ í¬ì°©
method='variance'      # ë¶„ì‚° ê¸°ë°˜

# í”¼ì²˜ ê°œìˆ˜
top_n=200  # ê¸°ë³¸ê°’
top_n=150  # ë” ì ê²Œ (ê³¼ì í•© ë°©ì§€)
top_n=300  # ë” ë§ì´ (ì •ë³´ ì†ì‹¤ ë°©ì§€)

# ìƒê´€ê´€ê³„ ì œê±°
corr_threshold=0.95  # ê¸°ë³¸ê°’
corr_threshold=0.90  # ë” ì—„ê²©í•˜ê²Œ
```

### í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë‹¨ê³„
```python
# íŠœë‹ ì‹œí–‰ íšŸìˆ˜
n_trials=50   # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
n_trials=100  # ê¸°ë³¸ê°’ (ê¶Œì¥)
n_trials=200  # ë” ì •ë°€í•˜ê²Œ

# ì‹œê°„ ì œí•œ
timeout=None      # ì œí•œ ì—†ìŒ
timeout=3600      # 1ì‹œê°„
timeout=7200      # 2ì‹œê°„
```

## ğŸ“Š ê²°ê³¼ í™•ì¸

### 1. ë¡œê·¸ í™•ì¸
```bash
tail -f logs/optimization.log
```

### 2. ê²°ê³¼ íŒŒì¼ ìœ„ì¹˜
```
artifacts/
  â”œâ”€â”€ lightgbm_best_params_optimized.json  # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°
  â””â”€â”€ models_optimized/                     # í•™ìŠµëœ ëª¨ë¸ë“¤

results/
  â”œâ”€â”€ feature_selection/
  â”‚   â””â”€â”€ selected_features_optimized.csv  # ì„ íƒëœ í”¼ì²˜ ëª©ë¡
  â”œâ”€â”€ interpretability_optimized/
  â”‚   â”œâ”€â”€ feature_importance_gain.csv      # í”¼ì²˜ ì¤‘ìš”ë„
  â”‚   â””â”€â”€ shap_importance.csv              # SHAP ì¤‘ìš”ë„
  â””â”€â”€ optimization/
      â””â”€â”€ optimization_summary.json        # ì „ì²´ ìš”ì•½
```

### 3. ì£¼ìš” ë©”íŠ¸ë¦­ í™•ì¸
```python
import json

# ìµœì í™” ê²°ê³¼ ë¡œë“œ
with open('results/optimization/optimization_summary.json', 'r') as f:
    results = json.load(f)

print(f"OOF Score: {results['final_model']['oof_score']:.6f}")
print(f"Best Tuning Score: {results['hyperparameter_tuning']['best_score']:.6f}")
print(f"Final Features: {results['feature_selection']['final_features']}")
```

## ğŸ”„ ë°˜ë³µ ì‹¤í—˜ ì „ëµ

### ì‹¤í—˜ 1: ë² ì´ìŠ¤ë¼ì¸ (ë¹ ë¥¸ ì‹¤í–‰)
```python
results = optimizer.run_full_optimization(
    # Preprocessing
    add_missing_indicators=True,
    handle_outliers=True,
    normalize=False,        # ë¹ ë¥´ê²Œ
    scale=True,
    # Feature Selection
    top_n_features=150,     # ì ê²Œ
    # Hyperparameter Tuning
    n_trials=20,            # ë¹ ë¥´ê²Œ
    calculate_shap=False
)
```

### ì‹¤í—˜ 2: ì¤‘ê°„ ë‹¨ê³„ (ê· í˜•)
```python
results = optimizer.run_full_optimization(
    # Preprocessing
    add_missing_indicators=True,
    add_regime_indicators=True,
    handle_outliers=True,
    normalize=True,
    scale=True,
    # Feature Selection
    top_n_features=200,
    # Hyperparameter Tuning
    n_trials=50,
    calculate_shap=False
)
```

### ì‹¤í—˜ 3: ì™„ì „ ìµœì í™” (ëŠë¦¼, ê³ ì„±ëŠ¥)
```python
results = optimizer.run_full_optimization(
    # Preprocessing
    add_missing_indicators=True,
    add_regime_indicators=True,
    handle_outliers=True,
    normalize=True,
    scale=True,
    # Feature Selection
    selection_method='mutual_info',  # ë” ì •ë°€
    top_n_features=300,
    # Hyperparameter Tuning
    n_trials=100,
    timeout=7200,  # 2ì‹œê°„
    calculate_shap=True  # SHAP ë¶„ì„
)
```

## ğŸ› ë¬¸ì œ í•´ê²°

### ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
# í”¼ì²˜ ê°œìˆ˜ ì¤„ì´ê¸°
top_n_features=100

# SHAP ê³„ì‚° ë„ê¸°
calculate_shap=False

# ìƒ˜í”Œë§ ì‚¬ìš©
# (tuner.py ë‚´ë¶€ì—ì„œ ìë™ ì²˜ë¦¬ë¨)
```

### í•™ìŠµ ì‹œê°„ì´ ë„ˆë¬´ ê¸º
```python
# íŠœë‹ ì‹œí–‰ ì¤„ì´ê¸°
n_trials=20

# ì‹œê°„ ì œí•œ ì„¤ì •
timeout=1800  # 30ë¶„

# í”¼ì²˜ ì„ íƒ ê°•í™”
top_n_features=100
```

### ê³¼ì í•© ë¬¸ì œ
```python
# í”¼ì²˜ ê°œìˆ˜ ì¤„ì´ê¸°
top_n_features=100

# ì •ê·œí™” ê°•í™” (configì—ì„œ ì¡°ì ˆ)
# learning_rate ë‚®ì¶”ê¸°
# min_data_in_leaf ë†’ì´ê¸°

# ìƒê´€ê´€ê³„ ì œê±° ê°•í™”
corr_threshold=0.90
```

## ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ íŒ

1. **ì „ì²˜ë¦¬ ì¡°í•© í…ŒìŠ¤íŠ¸**
   - ì •ê·œí™” ë°©ë²• ë³€ê²½: rank_gauss vs log1p vs rolling_zscore
   - ìœˆë„ìš° í¬ê¸° ì¡°ì ˆ: 30 vs 60 vs 90

2. **í”¼ì²˜ ì„ íƒ ì „ëµ**
   - correlationìœ¼ë¡œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸ í›„
   - mutual_infoë¡œ ì •ë°€í•˜ê²Œ ì¬ì„ íƒ

3. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰**
   - ì²« ì‹¤í–‰: n_trials=20 (ë¹ ë¥¸ íƒìƒ‰)
   - ë‘ ë²ˆì§¸: n_trials=50 (ì¤‘ê°„)
   - ìµœì¢…: n_trials=100+ (ì •ë°€)

4. **ì•™ìƒë¸” ì „ëµ**
   - ì—¬ëŸ¬ ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµ
   - ì˜ˆì¸¡ê°’ í‰ê·  ë˜ëŠ” ê°€ì¤‘ í‰ê· 

## ğŸ¯ ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ

| ë‹¨ê³„ | ì˜ˆìƒ ê°œì„  | ì‹œê°„ |
|-----|----------|------|
| ë² ì´ìŠ¤ë¼ì¸ (ê¸°ë³¸ ì„¤ì •) | - | 10ë¶„ |
| + ì „ì²˜ë¦¬ ìµœì í™” | +1~2% | 15ë¶„ |
| + í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ | +2~3% | 20ë¶„ |
| + í”¼ì²˜ ì„ íƒ | +1~2% | 25ë¶„ |
| + í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ | +2~4% | 1~2ì‹œê°„ |
| + SHAP ê¸°ë°˜ ì¬ì„ íƒ | +1~2% | ì¶”ê°€ 30ë¶„ |
| **ì´ ì˜ˆìƒ ê°œì„ ** | **7~13%** | **2~3ì‹œê°„** |

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ë°ì´í„° ë¡œë“œ í™•ì¸
- [ ] ì „ì²˜ë¦¬ ì„¤ì • í™•ì¸
- [ ] í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹¤í–‰
- [ ] í”¼ì²˜ ì„ íƒ ì™„ë£Œ
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ
- [ ] ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ
- [ ] OOF ìŠ¤ì½”ì–´ í™•ì¸
- [ ] í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
- [ ] ê²°ê³¼ ì €ì¥ í™•ì¸
- [ ] ëª¨ë¸ íŒŒì¼ ì €ì¥ í™•ì¸

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

```bash
# 1. ì˜ì¡´ì„± í™•ì¸
pip install optuna lightgbm scikit-learn pandas numpy

# 2. ì„¤ì • íŒŒì¼ í™•ì¸
cat conf/params.yaml

# 3. ìµœì í™” ì‹¤í–‰
python scripts/optimize_return_model.py

# 4. ê²°ê³¼ í™•ì¸
python -c "
import json
with open('results/optimization/optimization_summary.json', 'r') as f:
    print(json.dumps(json.load(f), indent=2))
"
```
