{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad0076b",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup: Configure Dataset & CV Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa2e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "DATASET_NAME = \"my-hull-models\"  # ‚Üê ÏóÖÎ°úÎìúÌïú dataset Ïù¥Î¶Ñ\n",
    "\n",
    "# ========== CV STRATEGY ÏÑ†ÌÉù (Ïó¨Í∏∞Îßå Î∞îÍæ∏ÏÑ∏Ïöî!) ==========\n",
    "CV_STRATEGY = \"time_based\"  # time_based, expanding_window, purged_walk_forward, regime_aware\n",
    "# ====================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"CV STRATEGY EVALUATION: {CV_STRATEGY.upper()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Kaggle Îç∞Ïù¥ÌÑ∞ÏÖã Í≤ΩÎ°ú\n",
    "DATASET_PATH = Path(f\"/kaggle/input/{DATASET_NAME}\")\n",
    "\n",
    "if DATASET_PATH.exists():\n",
    "    sys.path.insert(0, str(DATASET_PATH))\n",
    "    print(f\"‚úì Dataset found: {DATASET_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset not found: {DATASET_PATH}\")\n",
    "    input_dir = Path(\"/kaggle/input/\")\n",
    "    if input_dir.exists():\n",
    "        print(\"\\nüìÅ Available datasets:\")\n",
    "        for item in input_dir.iterdir():\n",
    "            print(f\"  - {item.name}\")\n",
    "    raise FileNotFoundError(f\"Dataset '{DATASET_NAME}' not found!\")\n",
    "\n",
    "# Config Î≥µÏÇ¨\n",
    "config_path = DATASET_PATH / \"conf\" / \"params.yaml\"\n",
    "if config_path.exists():\n",
    "    working_config_dir = Path(\"/kaggle/working/conf\")\n",
    "    working_config_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    import shutil\n",
    "    shutil.copy(config_path, working_config_dir / \"params.yaml\")\n",
    "    sys.path.insert(0, str(working_config_dir.parent))\n",
    "    print(f\"‚úì Config copied to: {working_config_dir}/params.yaml\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Config file not found: {config_path}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Setup complete! Using CV strategy: {CV_STRATEGY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5893ccb",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Train Models with Selected CV Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c68054",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'polars'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolars\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpl\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlgb\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'polars'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "\n",
    "from src.data import DataLoader\n",
    "from src.features import FeatureEngineering\n",
    "from src.cv import create_cv_strategy\n",
    "from src.metric import CompetitionMetric\n",
    "from src.position import SharpeScalingMapper\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"TRAINING WITH {CV_STRATEGY.upper()} CV\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== 1. Data Preparation ==========\n",
    "print(\"\\nüìä Step 1: Loading data...\")\n",
    "data_loader = DataLoader(\"conf/params.yaml\")\n",
    "train_df, _ = data_loader.load_data()\n",
    "print(f\"‚úì Loaded {len(train_df)} samples\")\n",
    "\n",
    "print(\"\\nüîß Step 2: Preprocessing...\")\n",
    "train_processed, _ = data_loader.preprocess_timeseries(\n",
    "    train_df,\n",
    "    handle_outliers=True,\n",
    "    normalize=True,\n",
    "    scale=True,\n",
    "    window=60\n",
    ")\n",
    "\n",
    "# Get feature columns (exclude metadata and target columns)\n",
    "exclude_cols = {'date_id', 'forward_returns', 'realized_vol', 'risk_free_rate'}\n",
    "feature_cols = [col for col in train_processed.columns if col not in exclude_cols]\n",
    "print(f\"\\n‚úì Using {len(feature_cols)} features from preprocessing\")\n",
    "\n",
    "# ========== 2. Create CV Strategy ==========\n",
    "print(f\"\\nüîÄ Step 3: Creating {CV_STRATEGY} CV strategy...\")\n",
    "cv_strategy = create_cv_strategy(\n",
    "    config_path=\"conf/params.yaml\",\n",
    "    strategy=CV_STRATEGY\n",
    ")\n",
    "folds = list(cv_strategy.get_folds(train_processed))\n",
    "print(f\"‚úì Created {len(folds)} folds\")\n",
    "\n",
    "# ========== 3. Model Parameters ==========\n",
    "return_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'verbosity': -1,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "\n",
    "risk_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 15,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'verbosity': -1,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "\n",
    "# ========== 4. Train Models ==========\n",
    "print(\"\\nüéØ Step 4: Training models...\")\n",
    "\n",
    "# Prepare realized volatility target\n",
    "# Sort by date_id to ensure chronological order\n",
    "train_processed = train_processed.sort_values('date_id').reset_index(drop=True)\n",
    "\n",
    "# Calculate rolling volatility over time (Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌï¥)\n",
    "train_processed['realized_vol'] = (\n",
    "    train_processed['forward_returns']\n",
    "    .rolling(window=30, min_periods=5)\n",
    "    .std()\n",
    ")\n",
    "\n",
    "# Fill remaining NaN values with mean\n",
    "train_processed['realized_vol'] = train_processed['realized_vol'].fillna(\n",
    "    train_processed['realized_vol'].mean()\n",
    ")\n",
    "\n",
    "print(f\"‚úì Realized volatility calculated (rolling window=20)\")\n",
    "print(f\"  Mean: {train_processed['realized_vol'].mean():.6f}\")\n",
    "print(f\"  Std: {train_processed['realized_vol'].std():.6f}\")\n",
    "print(f\"  NaN count: {train_processed['realized_vol'].isna().sum()}\")\n",
    "\n",
    "return_models = []\n",
    "risk_models = []\n",
    "oof_predictions = {\n",
    "    'r_hat': np.zeros(len(train_processed)),\n",
    "    'sigma_hat': np.zeros(len(train_processed)),\n",
    "    'allocations': np.zeros(len(train_processed)),\n",
    "    'mask': np.zeros(len(train_processed), dtype=bool)\n",
    "}\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(folds):\n",
    "    print(f\"\\n  üìÅ Fold {fold_idx + 1}/{len(folds)}\")\n",
    "    \n",
    "    # Return model\n",
    "    X_train = train_processed.iloc[train_idx][feature_cols]\n",
    "    y_train = train_processed.iloc[train_idx]['forward_returns']\n",
    "    X_val = train_processed.iloc[val_idx][feature_cols]\n",
    "    y_val = train_processed.iloc[val_idx]['forward_returns']\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    return_model = lgb.train(\n",
    "        return_params,\n",
    "        train_data,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=False)]\n",
    "    )\n",
    "    return_models.append(return_model)\n",
    "    r_hat = return_model.predict(X_val)\n",
    "    \n",
    "    # Risk model\n",
    "    y_train_vol = train_processed.iloc[train_idx]['realized_vol']\n",
    "    y_val_vol = train_processed.iloc[val_idx]['realized_vol']\n",
    "    \n",
    "    train_data_vol = lgb.Dataset(X_train, label=y_train_vol)\n",
    "    val_data_vol = lgb.Dataset(X_val, label=y_val_vol, reference=train_data_vol)\n",
    "    \n",
    "    risk_model = lgb.train(\n",
    "        risk_params,\n",
    "        train_data_vol,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=[val_data_vol],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=False)]\n",
    "    )\n",
    "    risk_models.append(risk_model)\n",
    "    sigma_hat = np.maximum(risk_model.predict(X_val), 1e-6)\n",
    "    \n",
    "    # Position strategy\n",
    "    position_mapper = SharpeScalingMapper(\"conf/params.yaml\")\n",
    "    allocations = position_mapper.map_positions(\n",
    "        r_hat=r_hat,\n",
    "        sigma_hat=sigma_hat,\n",
    "        k=1.0,\n",
    "        b=2.0\n",
    "    )\n",
    "    \n",
    "    # Store OOF\n",
    "    oof_predictions['r_hat'][val_idx] = r_hat\n",
    "    oof_predictions['sigma_hat'][val_idx] = sigma_hat\n",
    "    oof_predictions['allocations'][val_idx] = allocations\n",
    "    oof_predictions['mask'][val_idx] = True\n",
    "    \n",
    "    print(f\"    Return RMSE: {np.sqrt(np.mean((y_val - r_hat)**2)):.6f}\")\n",
    "    print(f\"    Risk RMSE: {np.sqrt(np.mean((y_val_vol - sigma_hat)**2)):.6f}\")\n",
    "\n",
    "# ========== 5. Calculate OOF Score ==========\n",
    "print(\"\\nüìä Step 5: Calculating OOF competition score...\")\n",
    "\n",
    "metric_calc = CompetitionMetric()\n",
    "oof_mask = oof_predictions['mask']\n",
    "\n",
    "oof_score = metric_calc.calculate_score(\n",
    "    allocations=oof_predictions['allocations'][oof_mask],\n",
    "    forward_returns=train_processed['forward_returns'].values[oof_mask],\n",
    "    risk_free_rate=train_processed['risk_free_rate'].values[oof_mask] if 'risk_free_rate' in train_processed.columns else None\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"OOF SCORE - {CV_STRATEGY.upper()}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Competition Score: {oof_score['score']:.6f}\")\n",
    "print(f\"  ‚Üí Sharpe Ratio: {oof_score['sharpe']:.6f}\")\n",
    "print(f\"  ‚Üí Vol Penalty: {oof_score['vol_penalty']:.4f}\")\n",
    "print(f\"  ‚Üí Return Penalty: {oof_score['return_penalty']:.4f}\")\n",
    "print(f\"  ‚Üí Vol Ratio: {oof_score['vol_ratio']:.4f}\")\n",
    "print(f\"Coverage: {oof_mask.sum() / len(train_processed):.2%}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== 6. Save Models & OOF Score ==========\n",
    "print(\"\\nüíæ Step 6: Saving models and results...\")\n",
    "\n",
    "model_dir = Path(\"/kaggle/working/artifacts\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save return models\n",
    "return_dir = model_dir / \"return_models\"\n",
    "return_dir.mkdir(exist_ok=True)\n",
    "for i, model in enumerate(return_models):\n",
    "    with open(return_dir / f\"model_fold_{i}.pkl\", 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "# Save risk models\n",
    "risk_dir = model_dir / \"risk_models\"\n",
    "risk_dir.mkdir(exist_ok=True)\n",
    "for i, model in enumerate(risk_models):\n",
    "    with open(risk_dir / f\"model_fold_{i}.pkl\", 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "# Save feature names for both return and risk models\n",
    "feature_info = {\n",
    "    'return_features': return_models[0].feature_name(),\n",
    "    'risk_features': risk_models[0].feature_name()\n",
    "}\n",
    "with open(model_dir / \"feature_names.json\", 'w') as f:\n",
    "    json.dump(feature_info, f)\n",
    "\n",
    "# Save OOF score\n",
    "with open(model_dir / \"oof_score.json\", 'w') as f:\n",
    "    json.dump({\n",
    "        'cv_strategy': CV_STRATEGY,\n",
    "        'oof_score': float(oof_score['score']),\n",
    "        'oof_sharpe': float(oof_score['sharpe']),\n",
    "        'oof_vol_penalty': float(oof_score['vol_penalty']),\n",
    "        'oof_return_penalty': float(oof_score['return_penalty']),\n",
    "        'oof_vol_ratio': float(oof_score['vol_ratio']),\n",
    "        'coverage': float(oof_mask.sum() / len(train_processed))\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Models saved to {model_dir}\")\n",
    "print(f\"‚úì Feature names saved:\")\n",
    "print(f\"  - Return features: {len(feature_info['return_features'])}\")\n",
    "print(f\"  - Risk features: {len(feature_info['risk_features'])}\")\n",
    "print(f\"‚úì OOF score saved: {oof_score['score']:.6f}\")\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bc57d2",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load Models & Start Inference Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e48daf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 3: Inference - Competition Server or Local Parquet ==========\n",
    "\n",
    "# Load feature names first\n",
    "with open(model_dir / \"feature_names.json\", 'r') as f:\n",
    "    feature_info = json.load(f)\n",
    "    return_feature_names = feature_info['return_features']\n",
    "    risk_feature_names = feature_info['risk_features']\n",
    "\n",
    "def predict(test, model=None):\n",
    "    \"\"\"\n",
    "    Prediction function for competition server.\n",
    "    \n",
    "    Args:\n",
    "        test: polars DataFrame with test features\n",
    "        model: Not used (models loaded globally)\n",
    "    \n",
    "    Returns:\n",
    "        float: allocation (0 to 2)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert to pandas\n",
    "        test_pd = test.to_pandas()\n",
    "        \n",
    "        # Preprocess test data (ÎèôÏùºÌïú Ï†ÑÏ≤òÎ¶¨ Ï†ÅÏö©)\n",
    "        data_loader = DataLoader(\"conf/params.yaml\")\n",
    "        test_processed, _ = data_loader.preprocess_timeseries(\n",
    "            test_pd,\n",
    "            handle_outliers=True,\n",
    "            normalize=True,\n",
    "            scale=True,\n",
    "            window=60\n",
    "        )\n",
    "        \n",
    "        # Prepare features for RETURN model (in exact training order)\n",
    "        X_test_return = pd.DataFrame(index=test_processed.index)\n",
    "        for feat in return_feature_names:\n",
    "            if feat in test_processed.columns:\n",
    "                X_test_return[feat] = test_processed[feat]\n",
    "            else:\n",
    "                X_test_return[feat] = 0.0\n",
    "        \n",
    "        # Prepare features for RISK model (in exact training order)\n",
    "        X_test_risk = pd.DataFrame(index=test_processed.index)\n",
    "        for feat in risk_feature_names:\n",
    "            if feat in test_processed.columns:\n",
    "                X_test_risk[feat] = test_processed[feat]\n",
    "            else:\n",
    "                X_test_risk[feat] = 0.0\n",
    "        \n",
    "        # Ensemble prediction - return (FIX: Ïò¨Î∞îÎ•∏ ÏïôÏÉÅÎ∏î)\n",
    "        r_hat_preds = np.array([model.predict(X_test_return) for model in return_models])\n",
    "        r_hat = float(np.mean(r_hat_preds, axis=0)[0])\n",
    "        \n",
    "        # Ensemble prediction - risk (FIX: Ïò¨Î∞îÎ•∏ ÏïôÏÉÅÎ∏î)\n",
    "        sigma_hat_preds = np.array([model.predict(X_test_risk) for model in risk_models])\n",
    "        sigma_hat = float(np.mean(sigma_hat_preds, axis=0)[0])\n",
    "        sigma_hat = max(sigma_hat, 1e-6)\n",
    "        \n",
    "        # Position mapping\n",
    "        mapper = SharpeScalingMapper()\n",
    "        allocation = mapper.map_positions(\n",
    "            r_hat=np.array([r_hat]),\n",
    "            sigma_hat=np.array([sigma_hat]),\n",
    "            k=1.0,\n",
    "            b=2.0\n",
    "        )[0]\n",
    "        \n",
    "        # Ensure bounds\n",
    "        allocation = max(0.0, min(2.0, float(allocation)))\n",
    "        \n",
    "        return allocation\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Prediction error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return 0.0\n",
    "\n",
    "print(\"‚úì Prediction function defined!\")\n",
    "\n",
    "# ========== Check if running in Kaggle environment ==========\n",
    "import os\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    # ========== COMPETITION MODE: Start Inference Server ==========\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üöÄ COMPETITION MODE - STARTING INFERENCE SERVER\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"CV Strategy: {CV_STRATEGY.upper()}\")\n",
    "    print(f\"OOF Score: {oof_score['score']:.6f}\")\n",
    "    print(f\"Models: {len(return_models)} folds\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    import kaggle_evaluation.default_inference_server\n",
    "    \n",
    "    inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "    inference_server.serve()\n",
    "    \n",
    "    print(\"\\n‚úÖ Inference server completed!\")\n",
    "\n",
    "else:\n",
    "    # ========== LOCAL MODE: Generate submission.parquet ==========\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üíª LOCAL MODE - GENERATING SUBMISSION.PARQUET\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Auto-detect test data path\n",
    "    input_dir = Path(\"/kaggle/input/\")\n",
    "    test_path = None\n",
    "    \n",
    "    # Search for test.csv in all input datasets\n",
    "    if input_dir.exists():\n",
    "        for dataset_dir in input_dir.iterdir():\n",
    "            if dataset_dir.is_dir():\n",
    "                potential_test = dataset_dir / \"test.csv\"\n",
    "                if potential_test.exists():\n",
    "                    test_path = potential_test\n",
    "                    print(f\"‚úì Found test data: {test_path}\")\n",
    "                    break\n",
    "    \n",
    "    if test_path is None or not test_path.exists():\n",
    "        print(\"‚ö†Ô∏è  Warning: Test file not found, using placeholder\")\n",
    "        submission = pl.DataFrame({\n",
    "            'date_id': range(100),\n",
    "            'allocation': [1.0] * 100\n",
    "        })\n",
    "    else:\n",
    "        test_data = pl.read_csv(test_path)\n",
    "        print(f\"‚úì Loaded test data: {len(test_data)} rows\")\n",
    "        \n",
    "        # Generate predictions\n",
    "        allocations = []\n",
    "        for idx in range(len(test_data)):\n",
    "            test_row = test_data[idx:idx+1]\n",
    "            allocation = predict(test_row)\n",
    "            allocations.append(allocation)\n",
    "        \n",
    "        # Create submission\n",
    "        submission = pl.DataFrame({\n",
    "            'date_id': test_data['date_id'].to_list(),\n",
    "            'allocation': allocations\n",
    "        })\n",
    "    \n",
    "    # Save to parquet\n",
    "    output_path = Path(\"/kaggle/working/submission.parquet\")\n",
    "    submission.write_parquet(output_path)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Submission saved to: {output_path}\")\n",
    "    print(f\"üìä Prediction Summary:\")\n",
    "    print(f\"  Mean allocation: {submission['allocation'].mean():.6f}\")\n",
    "    print(f\"  Std allocation: {submission['allocation'].std():.6f}\")\n",
    "    print(f\"  Min allocation: {submission['allocation'].min():.6f}\")\n",
    "    print(f\"  Max allocation: {submission['allocation'].max():.6f}\")\n",
    "    print(f\"\\nüîç First 5 predictions:\")\n",
    "    print(submission.head())\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
