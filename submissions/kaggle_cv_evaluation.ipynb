{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad0076b",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup: Configure Dataset & CV Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa2e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "DATASET_NAME = \"my-hull-models\"  # ‚Üê ÏóÖÎ°úÎìúÌïú dataset Ïù¥Î¶Ñ\n",
    "\n",
    "# ========== CV STRATEGY ÏÑ†ÌÉù (Ïó¨Í∏∞Îßå Î∞îÍæ∏ÏÑ∏Ïöî!) ==========\n",
    "CV_STRATEGY = \"time_based\"  # time_based, expanding_window, purged_walk_forward, regime_aware\n",
    "# ====================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"CV STRATEGY EVALUATION: {CV_STRATEGY.upper()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Kaggle Îç∞Ïù¥ÌÑ∞ÏÖã Í≤ΩÎ°ú\n",
    "DATASET_PATH = Path(f\"/kaggle/input/{DATASET_NAME}\")\n",
    "\n",
    "if DATASET_PATH.exists():\n",
    "    sys.path.insert(0, str(DATASET_PATH))\n",
    "    print(f\"‚úì Dataset found: {DATASET_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset not found: {DATASET_PATH}\")\n",
    "    input_dir = Path(\"/kaggle/input/\")\n",
    "    if input_dir.exists():\n",
    "        print(\"\\nüìÅ Available datasets:\")\n",
    "        for item in input_dir.iterdir():\n",
    "            print(f\"  - {item.name}\")\n",
    "    raise FileNotFoundError(f\"Dataset '{DATASET_NAME}' not found!\")\n",
    "\n",
    "# Config Î≥µÏÇ¨\n",
    "config_path = DATASET_PATH / \"conf\" / \"params.yaml\"\n",
    "if config_path.exists():\n",
    "    working_config_dir = Path(\"/kaggle/working/conf\")\n",
    "    working_config_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    import shutil\n",
    "    shutil.copy(config_path, working_config_dir / \"params.yaml\")\n",
    "    sys.path.insert(0, str(working_config_dir.parent))\n",
    "    print(f\"‚úì Config copied to: {working_config_dir}/params.yaml\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Config file not found: {config_path}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Setup complete! Using CV strategy: {CV_STRATEGY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5893ccb",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Train Models with Selected CV Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c68054",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'polars'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolars\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpl\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlgb\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'polars'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "\n",
    "from src.data import DataLoader\n",
    "from src.features import FeatureEngineering\n",
    "from src.cv import create_cv_strategy\n",
    "from src.metric import CompetitionMetric\n",
    "from src.position import SharpeScalingMapper\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"TRAINING WITH {CV_STRATEGY.upper()} CV\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== 1. Data Preparation ==========\n",
    "print(\"\\nüìä Step 1: Loading data...\")\n",
    "data_loader = DataLoader(\"conf/params.yaml\")\n",
    "train_df, _ = data_loader.load_data()\n",
    "print(f\"‚úì Loaded {len(train_df)} samples\")\n",
    "\n",
    "print(\"\\nüîß Step 2: Preprocessing...\")\n",
    "train_processed, _ = data_loader.preprocess_timeseries(\n",
    "    train_df,\n",
    "    handle_outliers=True,\n",
    "    normalize=True,\n",
    "    scale=True,\n",
    "    window=60\n",
    ")\n",
    "\n",
    "# Get feature columns (exclude metadata and target columns)\n",
    "exclude_cols = {'date_id', 'forward_returns', 'realized_vol', 'risk_free_rate'}\n",
    "feature_cols = [col for col in train_processed.columns if col not in exclude_cols]\n",
    "print(f\"\\n‚úì Using {len(feature_cols)} features from preprocessing\")\n",
    "\n",
    "# ========== 2. Create CV Strategy ==========\n",
    "print(f\"\\nüîÄ Step 3: Creating {CV_STRATEGY} CV strategy...\")\n",
    "cv_strategy = create_cv_strategy(\n",
    "    config_path=\"conf/params.yaml\",\n",
    "    strategy=CV_STRATEGY\n",
    ")\n",
    "folds = list(cv_strategy.get_folds(train_processed))\n",
    "print(f\"‚úì Created {len(folds)} folds\")\n",
    "\n",
    "# ========== 3. Model Parameters ==========\n",
    "return_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'verbosity': -1,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "\n",
    "risk_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 15,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'verbosity': -1,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "\n",
    "# ========== 4. Train Models ==========\n",
    "print(\"\\nüéØ Step 4: Training models...\")\n",
    "\n",
    "# Prepare realized volatility target\n",
    "# Sort by date_id to ensure chronological order\n",
    "train_processed = train_processed.sort_values('date_id').reset_index(drop=True)\n",
    "\n",
    "# Calculate rolling volatility over time (Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌï¥)\n",
    "train_processed['realized_vol'] = (\n",
    "    train_processed['forward_returns']\n",
    "    .rolling(window=30, min_periods=5)\n",
    "    .std()\n",
    ")\n",
    "\n",
    "# Fill remaining NaN values with mean\n",
    "train_processed['realized_vol'] = train_processed['realized_vol'].fillna(\n",
    "    train_processed['realized_vol'].mean()\n",
    ")\n",
    "\n",
    "print(f\"‚úì Realized volatility calculated (rolling window=20)\")\n",
    "print(f\"  Mean: {train_processed['realized_vol'].mean():.6f}\")\n",
    "print(f\"  Std: {train_processed['realized_vol'].std():.6f}\")\n",
    "print(f\"  NaN count: {train_processed['realized_vol'].isna().sum()}\")\n",
    "\n",
    "return_models = []\n",
    "risk_models = []\n",
    "oof_predictions = {\n",
    "    'r_hat': np.zeros(len(train_processed)),\n",
    "    'sigma_hat': np.zeros(len(train_processed)),\n",
    "    'allocations': np.zeros(len(train_processed)),\n",
    "    'mask': np.zeros(len(train_processed), dtype=bool)\n",
    "}\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(folds):\n",
    "    print(f\"\\n  üìÅ Fold {fold_idx + 1}/{len(folds)}\")\n",
    "    \n",
    "    # Return model\n",
    "    X_train = train_processed.iloc[train_idx][feature_cols]\n",
    "    y_train = train_processed.iloc[train_idx]['forward_returns']\n",
    "    X_val = train_processed.iloc[val_idx][feature_cols]\n",
    "    y_val = train_processed.iloc[val_idx]['forward_returns']\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    return_model = lgb.train(\n",
    "        return_params,\n",
    "        train_data,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=False)]\n",
    "    )\n",
    "    return_models.append(return_model)\n",
    "    r_hat = return_model.predict(X_val)\n",
    "    \n",
    "    # Risk model\n",
    "    y_train_vol = train_processed.iloc[train_idx]['realized_vol']\n",
    "    y_val_vol = train_processed.iloc[val_idx]['realized_vol']\n",
    "    \n",
    "    train_data_vol = lgb.Dataset(X_train, label=y_train_vol)\n",
    "    val_data_vol = lgb.Dataset(X_val, label=y_val_vol, reference=train_data_vol)\n",
    "    \n",
    "    risk_model = lgb.train(\n",
    "        risk_params,\n",
    "        train_data_vol,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=[val_data_vol],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=False)]\n",
    "    )\n",
    "    risk_models.append(risk_model)\n",
    "    sigma_hat = np.maximum(risk_model.predict(X_val), 1e-6)\n",
    "    \n",
    "    # Position strategy\n",
    "    position_mapper = SharpeScalingMapper(\"conf/params.yaml\")\n",
    "    allocations = position_mapper.map_positions(\n",
    "        r_hat=r_hat,\n",
    "        sigma_hat=sigma_hat,\n",
    "        k=1.0,\n",
    "        b=2.0\n",
    "    )\n",
    "    \n",
    "    # Store OOF\n",
    "    oof_predictions['r_hat'][val_idx] = r_hat\n",
    "    oof_predictions['sigma_hat'][val_idx] = sigma_hat\n",
    "    oof_predictions['allocations'][val_idx] = allocations\n",
    "    oof_predictions['mask'][val_idx] = True\n",
    "    \n",
    "    print(f\"    Return RMSE: {np.sqrt(np.mean((y_val - r_hat)**2)):.6f}\")\n",
    "    print(f\"    Risk RMSE: {np.sqrt(np.mean((y_val_vol - sigma_hat)**2)):.6f}\")\n",
    "\n",
    "# ========== 5. Calculate OOF Score ==========\n",
    "print(\"\\nüìä Step 5: Calculating OOF competition score...\")\n",
    "\n",
    "metric_calc = CompetitionMetric()\n",
    "oof_mask = oof_predictions['mask']\n",
    "\n",
    "oof_score = metric_calc.calculate_score(\n",
    "    allocations=oof_predictions['allocations'][oof_mask],\n",
    "    forward_returns=train_processed['forward_returns'].values[oof_mask],\n",
    "    risk_free_rate=train_processed['risk_free_rate'].values[oof_mask] if 'risk_free_rate' in train_processed.columns else None\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"OOF SCORE - {CV_STRATEGY.upper()}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Competition Score: {oof_score['score']:.6f}\")\n",
    "print(f\"  ‚Üí Sharpe Ratio: {oof_score['sharpe']:.6f}\")\n",
    "print(f\"  ‚Üí Vol Penalty: {oof_score['vol_penalty']:.4f}\")\n",
    "print(f\"  ‚Üí Return Penalty: {oof_score['return_penalty']:.4f}\")\n",
    "print(f\"  ‚Üí Vol Ratio: {oof_score['vol_ratio']:.4f}\")\n",
    "print(f\"Coverage: {oof_mask.sum() / len(train_processed):.2%}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== 6. Save Models & OOF Score ==========\n",
    "print(\"\\nüíæ Step 6: Saving models and results...\")\n",
    "\n",
    "model_dir = Path(\"/kaggle/working/artifacts\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save return models\n",
    "return_dir = model_dir / \"return_models\"\n",
    "return_dir.mkdir(exist_ok=True)\n",
    "for i, model in enumerate(return_models):\n",
    "    with open(return_dir / f\"model_fold_{i}.pkl\", 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "# Save risk models\n",
    "risk_dir = model_dir / \"risk_models\"\n",
    "risk_dir.mkdir(exist_ok=True)\n",
    "for i, model in enumerate(risk_models):\n",
    "    with open(risk_dir / f\"model_fold_{i}.pkl\", 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "# Save feature names for both return and risk models\n",
    "feature_info = {\n",
    "    'return_features': return_models[0].feature_name(),\n",
    "    'risk_features': risk_models[0].feature_name()\n",
    "}\n",
    "with open(model_dir / \"feature_names.json\", 'w') as f:\n",
    "    json.dump(feature_info, f)\n",
    "\n",
    "# Save OOF score\n",
    "with open(model_dir / \"oof_score.json\", 'w') as f:\n",
    "    json.dump({\n",
    "        'cv_strategy': CV_STRATEGY,\n",
    "        'oof_score': float(oof_score['score']),\n",
    "        'oof_sharpe': float(oof_score['sharpe']),\n",
    "        'oof_vol_penalty': float(oof_score['vol_penalty']),\n",
    "        'oof_return_penalty': float(oof_score['return_penalty']),\n",
    "        'oof_vol_ratio': float(oof_score['vol_ratio']),\n",
    "        'coverage': float(oof_mask.sum() / len(train_processed))\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Models saved to {model_dir}\")\n",
    "print(f\"‚úì Feature names saved:\")\n",
    "print(f\"  - Return features: {len(feature_info['return_features'])}\")\n",
    "print(f\"  - Risk features: {len(feature_info['risk_features'])}\")\n",
    "print(f\"‚úì OOF score saved: {oof_score['score']:.6f}\")\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bc57d2",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load Models & Start Inference Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e48daf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LOADING MODELS & GENERATING TEST PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data import DataLoader\n",
    "from src.position import SharpeScalingMapper\n",
    "\n",
    "# ========== Load Models ==========\n",
    "MODEL_DIR = Path(\"/kaggle/working/artifacts\")\n",
    "\n",
    "# Load return models (ÎèôÏ†ÅÏúºÎ°ú Î™®Îì† fold Î°úÎìú)\n",
    "return_models = []\n",
    "return_dir = MODEL_DIR / \"return_models\"\n",
    "if return_dir.exists():\n",
    "    # ÎîîÎ†âÌÜ†Î¶¨ÏóêÏÑú Î™®Îì† pkl ÌååÏùº Ï∞æÍ∏∞\n",
    "    model_files = sorted(return_dir.glob(\"model_fold_*.pkl\"))\n",
    "    for model_path in model_files:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            return_models.append(pickle.load(f))\n",
    "        print(f\"‚úì Loaded return model: {model_path.name}\")\n",
    "    print(f\"‚úì Total return models loaded: {len(return_models)}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Return models directory not found: {return_dir}\")\n",
    "\n",
    "# Load risk models (ÎèôÏ†ÅÏúºÎ°ú Î™®Îì† fold Î°úÎìú)\n",
    "risk_models = []\n",
    "risk_dir = MODEL_DIR / \"risk_models\"\n",
    "if risk_dir.exists():\n",
    "    # ÎîîÎ†âÌÜ†Î¶¨ÏóêÏÑú Î™®Îì† pkl ÌååÏùº Ï∞æÍ∏∞\n",
    "    model_files = sorted(risk_dir.glob(\"model_fold_*.pkl\"))\n",
    "    for model_path in model_files:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            risk_models.append(pickle.load(f))\n",
    "        print(f\"‚úì Loaded risk model: {model_path.name}\")\n",
    "    print(f\"‚úì Total risk models loaded: {len(risk_models)}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Risk models directory not found: {risk_dir}\")\n",
    "\n",
    "# Load feature names (returnÍ≥º risk Í∞ÅÍ∞Å)\n",
    "with open(MODEL_DIR / \"feature_names.json\", 'r') as f:\n",
    "    feature_info = json.load(f)\n",
    "    return_feature_names = feature_info['return_features']\n",
    "    risk_feature_names = feature_info['risk_features']\n",
    "\n",
    "print(f\"‚úì Loaded feature names:\")\n",
    "print(f\"  - Return features: {len(return_feature_names)}\")\n",
    "print(f\"  - Risk features: {len(risk_feature_names)}\")\n",
    "\n",
    "# Load OOF score for reference\n",
    "with open(MODEL_DIR / \"oof_score.json\", 'r') as f:\n",
    "    oof_info = json.load(f)\n",
    "\n",
    "print(f\"\\nüìä OOF Score (for comparison): {oof_info['oof_score']:.6f}\")\n",
    "\n",
    "# ========== Define Prediction Function ==========\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEFINING PREDICTION FUNCTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def predict(test: pl.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Kaggle API prediction function.\n",
    "    \n",
    "    Args:\n",
    "        test: Polars DataFrame with test features\n",
    "        \n",
    "    Returns:\n",
    "        allocation: float between 0.0 and 2.0\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert to pandas\n",
    "        test_pd = test.to_pandas()\n",
    "        \n",
    "        # Preprocess test data (ÎèôÏùºÌïú Ï†ÑÏ≤òÎ¶¨ Ï†ÅÏö©)\n",
    "        data_loader = DataLoader(\"conf/params.yaml\")\n",
    "        test_processed, _ = data_loader.preprocess_timeseries(\n",
    "            test_pd,\n",
    "            handle_outliers=True,\n",
    "            normalize=True,\n",
    "            scale=True,\n",
    "            window=60\n",
    "        )\n",
    "        \n",
    "        # Prepare features for RETURN model (in exact training order)\n",
    "        X_test_return = pd.DataFrame(index=test_processed.index)\n",
    "        for feat in return_feature_names:\n",
    "            if feat in test_processed.columns:\n",
    "                X_test_return[feat] = test_processed[feat]\n",
    "            else:\n",
    "                X_test_return[feat] = 0.0\n",
    "        \n",
    "        # Prepare features for RISK model (in exact training order)\n",
    "        X_test_risk = pd.DataFrame(index=test_processed.index)\n",
    "        for feat in risk_feature_names:\n",
    "            if feat in test_processed.columns:\n",
    "                X_test_risk[feat] = test_processed[feat]\n",
    "            else:\n",
    "                X_test_risk[feat] = 0.0\n",
    "        \n",
    "        # Ensemble prediction - return\n",
    "        r_hat_preds = [model.predict(X_test_return) for model in return_models]\n",
    "        r_hat = float(np.mean([np.mean(pred) for pred in r_hat_preds]))\n",
    "        \n",
    "        # Ensemble prediction - risk\n",
    "        sigma_hat_preds = [model.predict(X_test_risk) for model in risk_models]\n",
    "        sigma_hat = float(np.mean([np.mean(pred) for pred in sigma_hat_preds]))\n",
    "        sigma_hat = max(sigma_hat, 1e-6)  # Ensure positive\n",
    "        \n",
    "        # Position mapping\n",
    "        mapper = SharpeScalingMapper()\n",
    "        allocation = mapper.map_positions(\n",
    "            r_hat=np.array([r_hat]),\n",
    "            sigma_hat=np.array([sigma_hat]),\n",
    "            k=1.0,\n",
    "            b=2.0\n",
    "        )[0]\n",
    "        \n",
    "        # Ensure bounds\n",
    "        allocation = max(0.0, min(2.0, float(allocation)))\n",
    "        \n",
    "        return allocation\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Prediction error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return 0.0\n",
    "\n",
    "print(\"‚úì Prediction function defined!\")\n",
    "\n",
    "# ========== Check if running in Kaggle environment ==========\n",
    "import os\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    # ========== COMPETITION MODE: Start Inference Server ==========\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üöÄ COMPETITION MODE - STARTING INFERENCE SERVER\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"CV Strategy: {CV_STRATEGY.upper()}\")\n",
    "    print(f\"OOF Score: {oof_info['oof_score']:.6f}\")\n",
    "    print(f\"Models: {len(return_models)} folds\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    import kaggle_evaluation.default_inference_server\n",
    "    \n",
    "    inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "    inference_server.serve()\n",
    "    \n",
    "    print(\"\\n‚úÖ Inference server completed!\")\n",
    "\n",
    "else:\n",
    "    # ========== LOCAL MODE: Generate Predictions for Test Set ==========\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üß™ LOCAL MODE - GENERATING TEST PREDICTIONS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"CV Strategy: {CV_STRATEGY.upper()}\")\n",
    "    print(f\"OOF Score: {oof_info['oof_score']:.6f}\")\n",
    "    print(f\"Models: {len(return_models)} folds\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load test data\n",
    "    print(\"\\nüìä Loading test data...\")\n",
    "    data_loader = DataLoader(\"conf/params.yaml\")\n",
    "    _, test_df = data_loader.load_data()\n",
    "    print(f\"‚úì Loaded {len(test_df)} test samples\")\n",
    "    \n",
    "    # Predict for each test sample\n",
    "    print(\"\\nüéØ Generating predictions...\")\n",
    "    predictions = []\n",
    "    \n",
    "    for idx in range(len(test_df)):\n",
    "        # Get single row as polars DataFrame\n",
    "        test_row = pl.DataFrame(test_df.iloc[[idx]])\n",
    "        \n",
    "        # Predict\n",
    "        allocation = predict(test_row)\n",
    "        predictions.append(allocation)\n",
    "        \n",
    "        if (idx + 1) % 2 == 0 or idx == len(test_df) - 1:\n",
    "            print(f\"  Predicted {idx + 1}/{len(test_df)}: allocation = {allocation:.6f}\")\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'row_id': range(len(predictions)),\n",
    "        'allocation': predictions\n",
    "    })\n",
    "    \n",
    "    # Save as parquet\n",
    "    output_path = Path(\"/kaggle/working/submission.parquet\")\n",
    "    submission_df.to_parquet(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Predictions saved to: {output_path}\")\n",
    "    print(f\"\\nüìä Prediction Summary:\")\n",
    "    print(f\"  Mean allocation: {np.mean(predictions):.6f}\")\n",
    "    print(f\"  Std allocation: {np.std(predictions):.6f}\")\n",
    "    print(f\"  Min allocation: {np.min(predictions):.6f}\")\n",
    "    print(f\"  Max allocation: {np.max(predictions):.6f}\")\n",
    "    \n",
    "    # Display first few predictions\n",
    "    print(f\"\\nüîç First 5 predictions:\")\n",
    "    print(submission_df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä NEXT STEPS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"1. Submit this notebook to competition\")\n",
    "print(f\"2. Check Public LB score\")\n",
    "print(f\"3. Compare with OOF score: {oof_info['oof_score']:.6f}\")\n",
    "print(f\"4. Repeat for other CV strategies:\")\n",
    "print(f\"   - time_based\")\n",
    "print(f\"   - expanding_window\")\n",
    "print(f\"   - purged_walk_forward\")\n",
    "print(f\"   - regime_aware\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
