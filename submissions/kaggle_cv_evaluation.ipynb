{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad0076b",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Setup: Configure Dataset & CV Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa2e568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CV STRATEGY EVALUATION: TIME_BASED\n",
      "================================================================================\n",
      "âŒ Dataset not found: /kaggle/input/my-hull-models\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Dataset 'my-hull-models' not found!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m input_dir.iterdir():\n\u001b[32m     29\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Config ë³µì‚¬\u001b[39;00m\n\u001b[32m     33\u001b[39m config_path = DATASET_PATH / \u001b[33m\"\u001b[39m\u001b[33mconf\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mparams.yaml\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Dataset 'my-hull-models' not found!"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "DATASET_NAME = \"my-hull-models\"  # â† ì—…ë¡œë“œí•œ dataset ì´ë¦„\n",
    "\n",
    "# ========== CV STRATEGY ì„ íƒ (ì—¬ê¸°ë§Œ ë°”ê¾¸ì„¸ìš”!) ==========\n",
    "CV_STRATEGY = \"expanding_window\"  # time_based, expanding_window, purged_walk_forward, regime_aware\n",
    "# ====================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"CV STRATEGY EVALUATION: {CV_STRATEGY.upper()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Kaggle ë°ì´í„°ì…‹ ê²½ë¡œ\n",
    "DATASET_PATH = Path(f\"/kaggle/input/{DATASET_NAME}\")\n",
    "\n",
    "if DATASET_PATH.exists():\n",
    "    sys.path.insert(0, str(DATASET_PATH))\n",
    "    print(f\"âœ“ Dataset found: {DATASET_PATH}\")\n",
    "else:\n",
    "    print(f\"âŒ Dataset not found: {DATASET_PATH}\")\n",
    "    input_dir = Path(\"/kaggle/input/\")\n",
    "    if input_dir.exists():\n",
    "        print(\"\\nğŸ“ Available datasets:\")\n",
    "        for item in input_dir.iterdir():\n",
    "            print(f\"  - {item.name}\")\n",
    "    raise FileNotFoundError(f\"Dataset '{DATASET_NAME}' not found!\")\n",
    "\n",
    "# Config ë³µì‚¬\n",
    "config_path = DATASET_PATH / \"conf\" / \"params.yaml\"\n",
    "if config_path.exists():\n",
    "    working_config_dir = Path(\"/kaggle/working/conf\")\n",
    "    working_config_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    import shutil\n",
    "    shutil.copy(config_path, working_config_dir / \"params.yaml\")\n",
    "    sys.path.insert(0, str(working_config_dir.parent))\n",
    "    print(f\"âœ“ Config copied to: {working_config_dir}/params.yaml\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Config file not found: {config_path}\")\n",
    "\n",
    "print(f\"\\nâœ… Setup complete! Using CV strategy: {CV_STRATEGY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5893ccb",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Train Models with Selected CV Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c68054",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'polars'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolars\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpl\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlgb\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'polars'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "\n",
    "from src.data import DataLoader\n",
    "from src.features import FeatureEngineering\n",
    "from src.cv import create_cv_strategy\n",
    "from src.metric import CompetitionMetric\n",
    "from src.position import SharpeScalingMapper\n",
    "from src.tuner import OptunaLightGBMTuner\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"TRAINING WITH {CV_STRATEGY.upper()} CV\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== 1. Data Preparation ==========\n",
    "print(\"\\nğŸ“Š Step 1: Loading data...\")\n",
    "data_loader = DataLoader(\"conf/params.yaml\")\n",
    "train_df, _ = data_loader.load_data()\n",
    "print(f\"âœ“ Loaded {len(train_df)} samples\")\n",
    "\n",
    "print(\"\\nğŸ”§ Step 2: Preprocessing...\")\n",
    "train_processed, _ = data_loader.preprocess_timeseries(\n",
    "    train_df,\n",
    "    handle_outliers=False,\n",
    "    normalize=False,\n",
    "    scale=False,\n",
    "    window=60\n",
    ")\n",
    "is_feature_engineering = True  # Feature Engineering ìˆ˜í–‰ ì—¬ë¶€\n",
    "is_hyperparameter_tuning = False  # Hyperparameter Tuning ìˆ˜í–‰ ì—¬ë¶€ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)\n",
    "n_trials = 1\n",
    "\n",
    "# ========== 1.5 Prepare realized volatility target ==========\n",
    "# Sort by date_id to ensure chronological order\n",
    "train_processed = train_processed.sort_values('date_id').reset_index(drop=True)\n",
    "\n",
    "# Calculate rolling volatility over time\n",
    "train_processed['realized_vol'] = (\n",
    "    train_processed['forward_returns']\n",
    "    .rolling(window=30, min_periods=5)\n",
    "    .std()\n",
    ")\n",
    "\n",
    "# Fill remaining NaN values with mean\n",
    "train_processed['realized_vol'] = train_processed['realized_vol'].fillna(\n",
    "    train_processed['realized_vol'].mean()\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Realized volatility calculated (rolling window=30)\")\n",
    "print(f\"  Mean: {train_processed['realized_vol'].mean():.6f}\")\n",
    "print(f\"  Std: {train_processed['realized_vol'].std():.6f}\")\n",
    "print(f\"  NaN count: {train_processed['realized_vol'].isna().sum()}\")\n",
    "\n",
    "# ========== 1.6 Feature Engineering & Selection ==========\n",
    "print(\"\\nğŸ”§ Step 2.5: Feature engineering and selection...\")\n",
    "\n",
    "# Define metadata columns upfront\n",
    "metadata_cols = ['date_id', 'forward_returns', 'realized_vol']\n",
    "if 'risk_free_rate' in train_processed.columns:\n",
    "    metadata_cols.append('risk_free_rate')\n",
    "\n",
    "if not is_feature_engineering:\n",
    "    print(\"âš ï¸  í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤ (ì „ì²˜ë¦¬ëœ featureë§Œ ì‚¬ìš©)\")\n",
    "    \n",
    "    # All other columns are features\n",
    "    all_feature_cols = [col for col in train_processed.columns if col not in metadata_cols]\n",
    "    \n",
    "    # Return model: uses all features\n",
    "    return_feature_cols = all_feature_cols\n",
    "    \n",
    "    # Risk model: exclude forward_returns to prevent data leakage\n",
    "    risk_feature_cols = [col for col in all_feature_cols if col != 'forward_returns']\n",
    "    \n",
    "    # Use original data\n",
    "    train_df_final = train_processed\n",
    "    \n",
    "    print(f\"âœ“ Return ëª¨ë¸: {len(return_feature_cols)}ê°œ feature\")\n",
    "    print(f\"âœ“ Risk ëª¨ë¸: {len(risk_feature_cols)}ê°œ feature (forward_returns ì œì™¸)\")\n",
    "    \n",
    "else:\n",
    "    print(\"âœ… í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ & ì„ íƒ ìˆ˜í–‰\")\n",
    "    \n",
    "    # Feature engineering\n",
    "    feature_engineer = FeatureEngineering(\"conf/params.yaml\")\n",
    "    df_engineered = feature_engineer.fit_transform(train_processed)\n",
    "    \n",
    "    # Feature selection for return model\n",
    "    df_selected, _ = feature_engineer.select_features_by_importance(\n",
    "        df_engineered, \n",
    "        target_col='forward_returns'\n",
    "    )\n",
    "    df_final, _ = feature_engineer.remove_correlated_features(\n",
    "        df_selected, \n",
    "        target_col='forward_returns'\n",
    "    )\n",
    "    \n",
    "    # Get selected feature columns (everything except metadata)\n",
    "    selected_features = [col for col in df_final.columns if col not in metadata_cols]\n",
    "    \n",
    "    print(f\"\\nğŸ” ë””ë²„ê¹…: Feature selection ê²°ê³¼\")\n",
    "    print(f\"  df_final shape: {df_final.shape}\")\n",
    "    print(f\"  selected_features: {len(selected_features)}ê°œ\")\n",
    "    \n",
    "    # Build final dataframe: selected features from df_final + metadata from train_processed\n",
    "    train_df_final = df_final[selected_features].copy()\n",
    "    \n",
    "    # Add metadata/targets from original train_processed\n",
    "    for col in metadata_cols:\n",
    "        if col in train_processed.columns:\n",
    "            train_df_final[col] = train_processed[col].values\n",
    "    \n",
    "    # Return model: uses all selected features\n",
    "    return_feature_cols = selected_features\n",
    "    \n",
    "    # Risk model: exclude forward_returns to prevent data leakage\n",
    "    risk_feature_cols = [col for col in selected_features if col != 'forward_returns']\n",
    "    \n",
    "    print(f\"\\nâœ… ìµœì¢… ë°ì´í„°í”„ë ˆì„ êµ¬ì„±\")\n",
    "    print(f\"  Shape: {train_df_final.shape}\")\n",
    "    print(f\"  Return features: {len(return_feature_cols)}ê°œ\")\n",
    "    print(f\"  Risk features: {len(risk_feature_cols)}ê°œ\")\n",
    "    print(f\"  Metadata columns: {[c for c in metadata_cols if c in train_df_final.columns]}\")\n",
    "\n",
    "\n",
    "# ========== 2. Create CV Strategy ==========\n",
    "print(f\"\\nğŸ”€ Step 3: Creating {CV_STRATEGY} CV strategy...\")\n",
    "cv_strategy = create_cv_strategy(\n",
    "    config_path=\"conf/params.yaml\",\n",
    "    strategy=CV_STRATEGY\n",
    ")\n",
    "folds = list(cv_strategy.get_folds(train_df_final))\n",
    "print(f\"âœ“ Created {len(folds)} folds\")\n",
    "\n",
    "\n",
    "# ========== 3. Hyperparameter Tuning ==========\n",
    "if is_hyperparameter_tuning:\n",
    "    print(\"\\nğŸ” Step 4: Hyperparameter tuning...\")\n",
    "\n",
    "    print(\"\\n  ğŸ“ˆ Tuning return model...\")\n",
    "    return_tuner = OptunaLightGBMTuner(config_path=\"conf/params.yaml\", n_trials=n_trials, objective_type='return_ic')\n",
    "    best_params = return_tuner.optimize(\n",
    "        df=train_df_final,\n",
    "        feature_cols=return_feature_cols,\n",
    "        target_col='forward_returns',\n",
    "        study_name=f\"{CV_STRATEGY}_return_tuning\",\n",
    "    )\n",
    "\n",
    "    return_params = best_params if best_params else {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'verbosity': -1,\n",
    "        'n_jobs': 1\n",
    "    }\n",
    "\n",
    "    print(\"\\n  ğŸ“‰ Tuning risk model...\")\n",
    "    risk_tuner = OptunaLightGBMTuner(config_path=\"conf/params.yaml\", n_trials=n_trials, objective_type='risk_correlation')\n",
    "    best_risk_params = risk_tuner.optimize(\n",
    "        df=train_df_final,\n",
    "        feature_cols=risk_feature_cols,\n",
    "        target_col='realized_vol',\n",
    "        study_name=f\"{CV_STRATEGY}_risk_tuning\",\n",
    "    )\n",
    "\n",
    "    risk_params = best_risk_params if best_risk_params else {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 15,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'verbosity': -1,\n",
    "        'n_jobs': 1\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nâœ“ Hyperparameter tuning complete!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Step 4: Skipping hyperparameter tuning, using default parameters...\")\n",
    "    \n",
    "    # Default parameters for return model\n",
    "    return_params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'verbosity': -1,\n",
    "        'n_jobs': 1\n",
    "    }\n",
    "    \n",
    "    # Default parameters for risk model\n",
    "    risk_params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 15,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'verbosity': -1,\n",
    "        'n_jobs': 1\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ“ Using default parameters\")\n",
    "\n",
    "# ========== 4. Train Models ==========\n",
    "print(\"\\nğŸ¯ Step 5: Training models with selected parameters...\")\n",
    "print(f\"\\nğŸ” í•™ìŠµ ì „ ìµœì¢… ì²´í¬:\")\n",
    "print(f\"  train_df_final shape: {train_df_final.shape}\")\n",
    "print(f\"  train_df_final dtypes: {train_df_final.dtypes.value_counts().to_dict()}\")\n",
    "print(f\"  return_feature_cols: {len(return_feature_cols)}ê°œ\")\n",
    "print(f\"  risk_feature_cols: {len(risk_feature_cols)}ê°œ\")\n",
    "print(f\"  forward_returns in df: {'forward_returns' in train_df_final.columns}\")\n",
    "print(f\"  realized_vol in df: {'realized_vol' in train_df_final.columns}\")\n",
    "\n",
    "return_models = []\n",
    "risk_models = []\n",
    "oof_predictions = {\n",
    "    'r_hat': np.zeros(len(train_df_final)),\n",
    "    'sigma_hat': np.zeros(len(train_df_final)),\n",
    "    'allocations': np.zeros(len(train_df_final)),\n",
    "    'mask': np.zeros(len(train_df_final), dtype=bool)\n",
    "}\n",
    "\n",
    "# Store per-fold scores\n",
    "fold_scores = []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(folds):\n",
    "    print(f\"\\n  ğŸ“ Fold {fold_idx + 1}/{len(folds)}\")\n",
    "    \n",
    "    # Return model\n",
    "    X_train = train_df_final.iloc[train_idx][return_feature_cols]\n",
    "    y_train = train_df_final.iloc[train_idx]['forward_returns']\n",
    "    X_val = train_df_final.iloc[val_idx][return_feature_cols]\n",
    "    y_val = train_df_final.iloc[val_idx]['forward_returns']\n",
    "    \n",
    "    print(f\"    Return model - X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"    X_train dtypes: {X_train.dtypes.value_counts().to_dict()}\")\n",
    "    print(f\"    X_train sample stats:\")\n",
    "    print(f\"      - NaN counts: {X_train.isna().sum().sum()}\")\n",
    "    print(f\"      - Inf counts: {np.isinf(X_train.values).sum()}\")\n",
    "    print(f\"      - Constant cols (std=0): {(X_train.std() == 0).sum()}\")\n",
    "    print(f\"      - First 5 features: {X_train.columns[:5].tolist()}\")\n",
    "    print(f\"      - Sample values (first row, first 5 cols): {X_train.iloc[0, :5].values}\")\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    return_model = lgb.train(\n",
    "        return_params,\n",
    "        train_data,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=False)]\n",
    "    )\n",
    "    return_models.append(return_model)\n",
    "    r_hat = return_model.predict(X_val)\n",
    "    \n",
    "    # Risk model\n",
    "    X_train_risk = train_df_final.iloc[train_idx][risk_feature_cols]\n",
    "    y_train_vol = train_df_final.iloc[train_idx]['realized_vol']\n",
    "    X_val_risk = train_df_final.iloc[val_idx][risk_feature_cols]\n",
    "    y_val_vol = train_df_final.iloc[val_idx]['realized_vol']\n",
    "    \n",
    "    print(f\"    Risk model - X_train shape: {X_train_risk.shape}, y_train shape: {y_train_vol.shape}\")\n",
    "    \n",
    "    train_data_vol = lgb.Dataset(X_train_risk, label=y_train_vol)\n",
    "    val_data_vol = lgb.Dataset(X_val_risk, label=y_val_vol, reference=train_data_vol)\n",
    "    \n",
    "    risk_model = lgb.train(\n",
    "        risk_params,\n",
    "        train_data_vol,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=[val_data_vol],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=False)]\n",
    "    )\n",
    "    risk_models.append(risk_model)\n",
    "    sigma_hat = np.maximum(risk_model.predict(X_val_risk), 1e-6)\n",
    "    \n",
    "    # Position strategy\n",
    "    position_mapper = SharpeScalingMapper(\"conf/params.yaml\")\n",
    "    allocations = position_mapper.map_positions(\n",
    "        r_hat=r_hat,\n",
    "        sigma_hat=sigma_hat,\n",
    "        k=1.0,\n",
    "        b=2.0\n",
    "    )\n",
    "    \n",
    "    # Store OOF\n",
    "    oof_predictions['r_hat'][val_idx] = r_hat\n",
    "    oof_predictions['sigma_hat'][val_idx] = sigma_hat\n",
    "    oof_predictions['allocations'][val_idx] = allocations\n",
    "    oof_predictions['mask'][val_idx] = True\n",
    "    \n",
    "    # Calculate fold score\n",
    "    metric_calc = CompetitionMetric()\n",
    "    fold_forward_returns = train_df_final.iloc[val_idx]['forward_returns'].values\n",
    "    fold_risk_free = train_df_final.iloc[val_idx]['risk_free_rate'].values if 'risk_free_rate' in train_df_final.columns else None\n",
    "    \n",
    "    fold_score = metric_calc.calculate_score(\n",
    "        allocations=allocations,\n",
    "        forward_returns=fold_forward_returns,\n",
    "        risk_free_rate=fold_risk_free\n",
    "    )\n",
    "    fold_scores.append(fold_score)\n",
    "    \n",
    "    print(f\"    Return RMSE: {np.sqrt(np.mean((y_val - r_hat)**2)):.6f}\")\n",
    "    print(f\"    Risk RMSE: {np.sqrt(np.mean((y_val_vol - sigma_hat)**2)):.6f}\")\n",
    "    print(f\"    Fold Score: {fold_score['score']:.6f} (Sharpe: {fold_score['sharpe']:.6f})\")\n",
    "\n",
    "# ========== 5. Calculate OOF Score ==========\n",
    "print(\"\\nğŸ“Š Step 6: Calculating OOF competition score...\")\n",
    "\n",
    "metric_calc = CompetitionMetric()\n",
    "oof_mask = oof_predictions['mask']\n",
    "\n",
    "oof_score = metric_calc.calculate_score(\n",
    "    allocations=oof_predictions['allocations'][oof_mask],\n",
    "    forward_returns=train_df_final['forward_returns'].values[oof_mask],\n",
    "    risk_free_rate=train_df_final['risk_free_rate'].values[oof_mask] if 'risk_free_rate' in train_df_final.columns else None\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"OOF SCORE - {CV_STRATEGY.upper()}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nğŸ“Š Overall OOF Score:\")\n",
    "print(f\"Competition Score: {oof_score['score']:.6f}\")\n",
    "print(f\"  â†’ Sharpe Ratio: {oof_score['sharpe']:.6f}\")\n",
    "print(f\"  â†’ Vol Penalty: {oof_score['vol_penalty']:.4f}\")\n",
    "print(f\"  â†’ Return Penalty: {oof_score['return_penalty']:.4f}\")\n",
    "print(f\"  â†’ Vol Ratio: {oof_score['vol_ratio']:.4f}\")\n",
    "print(f\"Coverage: {oof_mask.sum() / len(train_df_final):.2%}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Per-Fold Scores:\")\n",
    "print(\"-\" * 80)\n",
    "fold_score_values = [fs['score'] for fs in fold_scores]\n",
    "fold_sharpe_values = [fs['sharpe'] for fs in fold_scores]\n",
    "for i, fs in enumerate(fold_scores):\n",
    "    print(f\"Fold {i+1}: Score={fs['score']:.6f}, Sharpe={fs['sharpe']:.6f}, Vol Penalty={fs['vol_penalty']:.4f}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Mean Â± Std: {np.mean(fold_score_values):.6f} Â± {np.std(fold_score_values):.6f}\")\n",
    "print(f\"Min / Max: {np.min(fold_score_values):.6f} / {np.max(fold_score_values):.6f}\")\n",
    "print(f\"Sharpe Mean Â± Std: {np.mean(fold_sharpe_values):.6f} Â± {np.std(fold_sharpe_values):.6f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== 6. Save Models & OOF Score ==========\n",
    "print(\"\\nğŸ’¾ Step 7: Saving models and results...\")\n",
    "\n",
    "model_dir = Path(\"/kaggle/working/artifacts\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save return models\n",
    "return_dir = model_dir / \"return_models\"\n",
    "return_dir.mkdir(exist_ok=True)\n",
    "for i, model in enumerate(return_models):\n",
    "    with open(return_dir / f\"model_fold_{i}.pkl\", 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "# Save risk models\n",
    "risk_dir = model_dir / \"risk_models\"\n",
    "risk_dir.mkdir(exist_ok=True)\n",
    "for i, model in enumerate(risk_models):\n",
    "    with open(risk_dir / f\"model_fold_{i}.pkl\", 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "# Save feature names for both return and risk models\n",
    "feature_info = {\n",
    "    'return_features': return_models[0].feature_name(),\n",
    "    'risk_features': risk_models[0].feature_name()\n",
    "}\n",
    "with open(model_dir / \"feature_names.json\", 'w') as f:\n",
    "    json.dump(feature_info, f)\n",
    "\n",
    "# Save OOF score with fold-level details\n",
    "with open(model_dir / \"oof_score.json\", 'w') as f:\n",
    "    json.dump({\n",
    "        'cv_strategy': CV_STRATEGY,\n",
    "        'oof_score': float(oof_score['score']),\n",
    "        'oof_sharpe': float(oof_score['sharpe']),\n",
    "        'oof_vol_penalty': float(oof_score['vol_penalty']),\n",
    "        'oof_return_penalty': float(oof_score['return_penalty']),\n",
    "        'oof_vol_ratio': float(oof_score['vol_ratio']),\n",
    "        'coverage': float(oof_mask.sum() / len(train_df_final)),\n",
    "        'fold_scores': [\n",
    "            {\n",
    "                'fold': i + 1,\n",
    "                'score': float(fs['score']),\n",
    "                'sharpe': float(fs['sharpe']),\n",
    "                'vol_penalty': float(fs['vol_penalty']),\n",
    "                'return_penalty': float(fs['return_penalty']),\n",
    "                'vol_ratio': float(fs['vol_ratio'])\n",
    "            }\n",
    "            for i, fs in enumerate(fold_scores)\n",
    "        ],\n",
    "        'fold_score_stats': {\n",
    "            'mean': float(np.mean(fold_score_values)),\n",
    "            'std': float(np.std(fold_score_values)),\n",
    "            'min': float(np.min(fold_score_values)),\n",
    "            'max': float(np.max(fold_score_values)),\n",
    "            'sharpe_mean': float(np.mean(fold_sharpe_values)),\n",
    "            'sharpe_std': float(np.std(fold_sharpe_values))\n",
    "        }\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Models saved to {model_dir}\")\n",
    "print(f\"âœ“ Feature names saved:\")\n",
    "print(f\"  - Return features: {len(feature_info['return_features'])}\")\n",
    "print(f\"  - Risk features: {len(feature_info['risk_features'])}\")\n",
    "print(f\"âœ“ OOF score saved: {oof_score['score']:.6f}\")\n",
    "print(f\"âœ“ Fold-level scores saved\")\n",
    "print(\"\\nâœ… Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bc57d2",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Load Models & Start Inference Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e48daf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Cell 3: Inference - Competition Server or Local Parquet ==========\n",
    "\n",
    "# Load feature names first\n",
    "with open(model_dir / \"feature_names.json\", 'r') as f:\n",
    "    feature_info = json.load(f)\n",
    "    return_feature_names = feature_info['return_features']\n",
    "    risk_feature_names = feature_info['risk_features']\n",
    "\n",
    "def predict(test, model=None):\n",
    "    \"\"\"\n",
    "    Prediction function for competition server.\n",
    "    \n",
    "    Args:\n",
    "        test: polars DataFrame with test features\n",
    "        model: Not used (models loaded globally)\n",
    "    \n",
    "    Returns:\n",
    "        float: allocation (0 to 2)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert to pandas\n",
    "        test_pd = test.to_pandas()\n",
    "        \n",
    "        # Preprocess test data (ë™ì¼í•œ ì „ì²˜ë¦¬ ì ìš©)\n",
    "        data_loader = DataLoader(\"conf/params.yaml\")\n",
    "        test_processed, _ = data_loader.preprocess_timeseries(\n",
    "            test_pd,\n",
    "            handle_outliers=True,\n",
    "            normalize=True,\n",
    "            scale=True,\n",
    "            window=60\n",
    "        )\n",
    "        \n",
    "        # Prepare features for RETURN model (in exact training order)\n",
    "        X_test_return = pd.DataFrame(index=test_processed.index)\n",
    "        for feat in return_feature_names:\n",
    "            if feat in test_processed.columns:\n",
    "                X_test_return[feat] = test_processed[feat]\n",
    "            else:\n",
    "                X_test_return[feat] = 0.0\n",
    "        \n",
    "        # Prepare features for RISK model (in exact training order)\n",
    "        X_test_risk = pd.DataFrame(index=test_processed.index)\n",
    "        for feat in risk_feature_names:\n",
    "            if feat in test_processed.columns:\n",
    "                X_test_risk[feat] = test_processed[feat]\n",
    "            else:\n",
    "                X_test_risk[feat] = 0.0\n",
    "        \n",
    "        # Ensemble prediction - return (FIX: ì˜¬ë°”ë¥¸ ì•™ìƒë¸”)\n",
    "        r_hat_preds = np.array([model.predict(X_test_return) for model in return_models])\n",
    "        r_hat = float(np.mean(r_hat_preds, axis=0)[0])\n",
    "        \n",
    "        # Ensemble prediction - risk (FIX: ì˜¬ë°”ë¥¸ ì•™ìƒë¸”)\n",
    "        sigma_hat_preds = np.array([model.predict(X_test_risk) for model in risk_models])\n",
    "        sigma_hat = float(np.mean(sigma_hat_preds, axis=0)[0])\n",
    "        sigma_hat = max(sigma_hat, 1e-6)\n",
    "        \n",
    "        # Position mapping\n",
    "        mapper = SharpeScalingMapper()\n",
    "        allocation = mapper.map_positions(\n",
    "            r_hat=np.array([r_hat]),\n",
    "            sigma_hat=np.array([sigma_hat]),\n",
    "            k=1.0,\n",
    "            b=2.0\n",
    "        )[0]\n",
    "        \n",
    "        # Ensure bounds\n",
    "        allocation = max(0.0, min(2.0, float(allocation)))\n",
    "        \n",
    "        return allocation\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Prediction error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return 0.0\n",
    "\n",
    "print(\"âœ“ Prediction function defined!\")\n",
    "\n",
    "# ========== Check if running in Kaggle environment ==========\n",
    "import os\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    # ========== COMPETITION MODE: Start Inference Server ==========\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸš€ COMPETITION MODE - STARTING INFERENCE SERVER\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"CV Strategy: {CV_STRATEGY.upper()}\")\n",
    "    print(f\"OOF Score: {oof_score['score']:.6f}\")\n",
    "    print(f\"Models: {len(return_models)} folds\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    import kaggle_evaluation.default_inference_server\n",
    "    \n",
    "    inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "    inference_server.serve()\n",
    "    \n",
    "    print(\"\\nâœ… Inference server completed!\")\n",
    "\n",
    "else:\n",
    "    # ========== LOCAL MODE: Generate submission.parquet ==========\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ’» LOCAL MODE - GENERATING SUBMISSION.PARQUET\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Auto-detect test data path\n",
    "    input_dir = Path(\"/kaggle/input/\")\n",
    "    test_path = None\n",
    "    \n",
    "    # Search for test.csv in all input datasets\n",
    "    if input_dir.exists():\n",
    "        for dataset_dir in input_dir.iterdir():\n",
    "            if dataset_dir.is_dir():\n",
    "                potential_test = dataset_dir / \"test.csv\"\n",
    "                if potential_test.exists():\n",
    "                    test_path = potential_test\n",
    "                    print(f\"âœ“ Found test data: {test_path}\")\n",
    "                    break\n",
    "    \n",
    "    if test_path is None or not test_path.exists():\n",
    "        print(\"âš ï¸  Warning: Test file not found, using placeholder\")\n",
    "        submission = pl.DataFrame({\n",
    "            'date_id': range(100),\n",
    "            'allocation': [1.0] * 100\n",
    "        })\n",
    "    else:\n",
    "        test_data = pl.read_csv(test_path)\n",
    "        print(f\"âœ“ Loaded test data: {len(test_data)} rows\")\n",
    "        \n",
    "        # Generate predictions\n",
    "        allocations = []\n",
    "        for idx in range(len(test_data)):\n",
    "            test_row = test_data[idx:idx+1]\n",
    "            allocation = predict(test_row)\n",
    "            allocations.append(allocation)\n",
    "        \n",
    "        # Create submission\n",
    "        submission = pl.DataFrame({\n",
    "            'date_id': test_data['date_id'].to_list(),\n",
    "            'allocation': allocations\n",
    "        })\n",
    "    \n",
    "    # Save to parquet\n",
    "    output_path = Path(\"/kaggle/working/submission.parquet\")\n",
    "    submission.write_parquet(output_path)\n",
    "    \n",
    "    print(f\"\\nâœ… Submission saved to: {output_path}\")\n",
    "    print(f\"ğŸ“Š Prediction Summary:\")\n",
    "    print(f\"  Mean allocation: {submission['allocation'].mean():.6f}\")\n",
    "    print(f\"  Std allocation: {submission['allocation'].std():.6f}\")\n",
    "    print(f\"  Min allocation: {submission['allocation'].min():.6f}\")\n",
    "    print(f\"  Max allocation: {submission['allocation'].max():.6f}\")\n",
    "    print(f\"\\nğŸ” First 5 predictions:\")\n",
    "    print(submission.head())\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
